{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datalib import InputPath\n",
    "\n",
    "index_path = InputPath(\"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_812f9079_3f98_4da2_9bb1_b25683f79f55\").resolve()\n",
    "index_path = index_path / \"**/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4036119/1014866433.py:2: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  adadeep.columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['synthesia_id',\n",
       " 'clip_id',\n",
       " 'clip_type',\n",
       " 'c_start_frame',\n",
       " 'c_end_frame',\n",
       " 'c_duration_s',\n",
       " 'c_frame_rate',\n",
       " 'c_video_width',\n",
       " 'c_video_height',\n",
       " 'camera',\n",
       " 'video_s3_path',\n",
       " 'audio_embedding_s3_path',\n",
       " 'landmarks_s3_path',\n",
       " 'landmarks_3d_s3_path',\n",
       " 'calibration_s3_path',\n",
       " 'hand_tracking_s3_path',\n",
       " 'eyelandmarks_s3_path',\n",
       " 'video_weka_path',\n",
       " 'landmarks_weka_path',\n",
       " 'audio_embedding_weka_path',\n",
       " 'landmarks_3d_weka_path',\n",
       " 'calibration_weka_path',\n",
       " 'hand_tracking_weka_path',\n",
       " 'eyelandmarks_weka_path',\n",
       " 'split',\n",
       " 'actor_free_description',\n",
       " 'actor_profession_description',\n",
       " 'emotion_body_language',\n",
       " 'emotion_language',\n",
       " 'emotion_overall_emotion_description',\n",
       " 'environment_description',\n",
       " 'actor_actions_description',\n",
       " 'actor_blocking_description',\n",
       " 'actor_interaction_description',\n",
       " 'camera_framing_description']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adadeep = pl.scan_parquet(index_path)\n",
    "adadeep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_STOCK_PERSONAL_AVATAR_INDEXES = {\n",
    "    \"aaron_1\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_3aa5c993_6412_4d85_b264_9931d37e6ba4\",\n",
    "    \"aaron_2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_4deda56c_f62a_40b6_9ecf_eb91234b45fe\",\n",
    "    \"aaron_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_4072d9ae_e579_404a_aa24_c1703738bd6d\",\n",
    "    \"aaron_4\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_eadaec5f_0f58_48bb_a109_69e9733be310\",\n",
    "    \"anna_m_1\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_05c2a33f_c468_4a84_be06_57a937d968d3\",\n",
    "    \"anna_m_2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_99ee283c_d219_48bd_bd8c_c6f9dedb8dfe\",\n",
    "    \"anna_m_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_7d71f945_d313_44cf_aa25_7ca9f41e80f6\",\n",
    "    \"anna_m_4\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_7b1eef6d_510a_4ff0_88f7_2419c0d8cf2a\",\n",
    "    \"anna_m_5\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_fdbe4939_3f07_43b9_9b49_a313d66db983\",\n",
    "    \"anna_m_6\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_b0dfd37b_aba2_4c3b_a6fe_410fc038df23\",\n",
    "    \"anna_m_7\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_b3ef3e9f_12ee_48d0_b1d1_badf243733f7\",\n",
    "    \"anna_m_8\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_61486d19_58ec_4867_ab21_9bf53db67362\",\n",
    "    \"anna_m_9\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_6ec0ac8f_e05c_45ea_804c_8d985c3d6d2d\",\n",
    "    \"anna_m_10\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_4ac57eb8_36fc_442f_9790_c3e077107643\",\n",
    "    \"anna_m_11\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_dcb489ed_50ff_49c1_836d_343feee9a18a\",\n",
    "    \"ava_1_v2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_8b5d9df7_ad4b_4b16_8a7b_6f3541362805\",\n",
    "    \"ava_2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_62f29a4a_864b_4e0e_8bba_e62b8803be13\",\n",
    "    \"ava_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f16a8fd8_b16e_491f_b5d5_f188922b76b2\",\n",
    "    \"ava_4_v2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_d3464cec_4596_4f8b_9af0_91dae91deecf\",\n",
    "    \"ava_5\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_82bc5f39_c4eb_429c_ae6d_feab053f179d\",\n",
    "    \"ava_6\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_984a9629_1ff7_47d7_9361_ce7d55209cf1\",\n",
    "    \"dan_r_1\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_08da43e7_8ef7_4454_948f_03d440fbc3c3\",\n",
    "    \"dan_r_2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_45b8596b_a78b_429a_90a5_26f1d751d311\",\n",
    "    \"georgina_kj_1\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_c4b11b07_75dc_4607_8566_7ee0482082b5\",\n",
    "    \"georgina_kj_2_v2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_1b5a0bfe_73d4_44cb_aa60_6018e475208b\",\n",
    "    \"georgina_kj_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_3aef886b_185b_4e2d_8ed7_c85a24cbfdc5\",\n",
    "    \"georgina_kj_4\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_a8df86ab_e31e_49b4_bb56_eb454bb835f7\",\n",
    "    \"georgina_kj_5\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_ef70b047_486d_4dcd_a0be_006fbba4a684\",\n",
    "    \"matilda_v_1\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_a6f8c378_0dc0_42ee_bc78_39bbc2bb36fe\",\n",
    "    \"matilda_v_2\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_4d1f0caf_3fca_4cc8_83d2_68c1d41ac470\",\n",
    "    \"matilda_v_2_gaze\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_a3c22cac_0573_4c01_8ef2_263b77e9a515\",\n",
    "    \"matilda_v_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_dcf8e599_4a14_438d_8cd3_87d89df9b0d0\",\n",
    "    \"matilda_v_3_gaze\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_dc5662db_f273_42e7_965d_b4c2a0b7e6ea\",\n",
    "    \"tat_ash_2_v3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_3de594d4_c1d8_4565_88ee_f8657831e0d5\",\n",
    "    \"tat_ash_3\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_d747f657_30d1_4319_b5d7_9d6fc067bae0\",\n",
    "    \"tat_ash_4\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_2a3bc014_32c0_4de0_9ad3_df1cd82c33cf\",\n",
    "    \"tat_ash_5\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_cf2e5bf4_efcc_4adc_a50f_81a888eb9182\",\n",
    "    \"tat_ash_6\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_6e8fcae9_5e02_4376_835e_370dcc356867\",\n",
    "    \"tat_ash_7\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_9f552356_850e_46d5_ba4a_3b17b52c9d1b\",\n",
    "    \"tat_ash_8\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_b4b8bf58_9373_4492_9c17_4e50f179302c\",\n",
    "    \"tat_ash_9\": \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_839d73e1_ff2c_4356_8b53_a72948d0a9a4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_PERSONAL_AVATAR_PROMPTS = {\n",
    "    \"aaron_1\": \"The image shows a person in a white button-down shirt standing in what appears to be an upscale bar or \"\n",
    "    \"restaurant setting, with a warm golden-toned interior featuring bar stools, shelves of bottles, and modern decor \"\n",
    "    \"including a geometric brass or gold-colored counter element. The person is standing in a welcoming, professional \"\n",
    "    \"pose with hands clasped in front, appearing to be speaking or presenting with an engaging, friendly expression. \"\n",
    "    \"The image is static and doesn't show any movement, though the person's posture and expression suggest they could \"\n",
    "    \"be a host, manager, or hospitality professional addressing someone off-camera.\",\n",
    "    #\n",
    "    \"aaron_2\": \"The image shows a person in a professional setting at what appears to be an upscale bar or restaurant, \"\n",
    "    \"wearing a crisp white button-down shirt, sitting casually at a dark marble or stone bar counter with bar stools \"\n",
    "    \"visible, a laptop to their side, and wine glasses lined up along the bar in the background, while they gesture \"\n",
    "    \"naturally with clasped hands while appearing to be engaged in conversation, with warm wood ceiling features and \"\n",
    "    \"blue walls creating an elegant ambiance.\",\n",
    "    #\n",
    "    \"anna_m_1\": \"The image shows a person sitting casually on a couch, wearing denim overalls over a black and white \"\n",
    "    \"striped long-sleeve shirt, with a relaxed yet attentive posture suggesting gentle, deliberate movements - their \"\n",
    "    \"hands are clasped together in their lap, their shoulders are at ease, and their composed facial expression and \"\n",
    "    \"direct gaze convey a sense of calm confidence, while the home setting includes a textured white wall, a \"\n",
    "    \"houseplant, and decorative pillows with geometric patterns.\",\n",
    "    #\n",
    "    \"ava_1_v2\": \"The image shows a person in a modern kitchen setting wearing a striped apron and light gray blouse, \"\n",
    "    \"standing behind a stovetop with a large pot, appearing to be filming a cooking demonstration or tutorial. The \"\n",
    "    \"kitchen has warm lighting with glass jars on shelves above, a black faucet, wooden bowls of citrus fruits, and \"\n",
    "    \"various cooking implements including a mortar and pestle. The person maintains a friendly, engaging expression \"\n",
    "    \"while standing still and gesturing naturally while speaking, though this is a still image rather than showing \"\n",
    "    \"actual movement.\",\n",
    "    #\n",
    "    \"ava_4_v2\": \"The image shows a professional in a pink blazer seated at a wooden desk in what appears to be a home \"\n",
    "    \"office setting, with a laptop, wireless keyboard, and mouse visible. The person appears confident and engaged, \"\n",
    "    \"with a warm, genuine smile and relaxed posture that suggests natural, fluid movements. The background features a \"\n",
    "    \"window with lush greenery outside and decorative elements including a stylish wooden lamp with a white shade and \"\n",
    "    \"glass decor on the side table. The lighting is bright and natural, creating a welcoming workspace atmosphere.\",\n",
    "    #\n",
    "    \"dan_r_1\": \"The image shows someone seated at a desk in what appears to be a home office or study, wearing a blue \"\n",
    "    \"quarter-zip sweater over a white collared shirt, positioned in front of bookshelves filled with hardbound books, \"\n",
    "    \"with a laptop and coffee cup on the desk, and an acoustic guitar mounted on the wall - while this appears to be a \"\n",
    "    \"still image, their body language suggests a relaxed yet professional demeanor with clasped hands and a direct \"\n",
    "    \"gaze toward the camera, suggesting someone who likely moves with deliberate, measured gestures in keeping with \"\n",
    "    \"their professional presentation.\",\n",
    "    #\n",
    "    \"georgina_kj_1\": \"The image shows a person having a summer picnic outdoors, seated on a striped blanket on green \"\n",
    "    \"grass, wearing a flowing white sundress with a square neckline and cinched waist - they appear relaxed and \"\n",
    "    \"cheerful with natural, animated gestures while speaking, with a picnic setup including a wicker basket, straw \"\n",
    "    \"hat, refreshments, and food plates arranged around them, while the background features lush green trees and a \"\n",
    "    \"building facade, suggesting gentle, engaging movements as they might gesture toward the picnic items or adjust \"\n",
    "    \"their position on the blanket, maintaining an authentic and warm presence that matches the sunny, casual outdoor \"\n",
    "    \"setting.\",\n",
    "    #\n",
    "    \"georgina_kj_2_v2\": \"The image shows a peaceful summer picnic scene with a person in a flowing white sundress \"\n",
    "    \"sitting gracefully on a striped blanket beneath a large tree, their posture poised and elegant with hands folded \"\n",
    "    \"neatly in their lap, suggesting controlled and refined movements. The picnic setting is complete with a woven \"\n",
    "    \"picnic basket, pink roses, a bottle of what appears to be lemonade, and a small white dog resting beside them, \"\n",
    "    \"while a straw hat rests at the corner of the blanket and metal containers sit nearby, all captured in warm, \"\n",
    "    \"natural lighting that creates dappled shadows through the tree's canopy.\",\n",
    "    #\n",
    "    \"matilda_v_1\": \"The image shows a professional in a crisp white blazer seated at a dark desk in what appears to be \"\n",
    "    \"an elegant office, with a striking abstract artwork featuring blue and yellow tones hanging on the wall behind \"\n",
    "    \"them, and vintage typewriters mounted as decorative pieces on either side. Based on the composed, confident \"\n",
    "    \"posture with clasped hands and straight-backed seated position, this person likely moves with deliberate grace \"\n",
    "    \"and poise, suggesting measured, purposeful movements that convey executive presence. The desk setup includes what \"\n",
    "    \"appears to be paperwork, a pen, and a coffee cup, arranged in an orderly fashion that suggests methodical, \"\n",
    "    \"organized movement patterns in their workspace.\",\n",
    "    #\n",
    "    \"tat_ash_2_v3\": \"The image shows a professional office or workspace setting with two people engaged in \"\n",
    "    \"conversation - one person with long black braids wearing a light-colored outfit is shown from behind, while \"\n",
    "    \"across from them sits a person in a navy pinstriped blazer and white t-shirt who is smiling warmly and has their \"\n",
    "    \"hands clasped while appearing engaged in the discussion; in the background, there's a modern open office \"\n",
    "    \"environment with desks, monitors, desk lamps, and other employees working, with large windows letting in natural \"\n",
    "    \"light.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  5.12it/s]\n",
      "/tmp/ipykernel_4036119/1224133096.py:11: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  merged_schema = adadeep.schema\n",
      "/tmp/ipykernel_4036119/1224133096.py:12: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  merged_schema.update(spa_df.schema)\n",
      "/tmp/ipykernel_4036119/1224133096.py:15: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols1 = set(adadeep.columns)\n",
      "/tmp/ipykernel_4036119/1224133096.py:16: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  cols2 = set(spa_df.columns)\n",
      "/tmp/ipykernel_4036119/1224133096.py:21: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in adadeep.columns:\n",
      "/tmp/ipykernel_4036119/1224133096.py:23: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  if col not in spa_df.columns:\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.94it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  5.13it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  5.00it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  5.14it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.93it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.71it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.98it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.96it/s]\n",
      "Downloading files: 100%|██████████| 3/3 [00:00<00:00,  4.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for avatar_name, index_path in ALL_STOCK_PERSONAL_AVATAR_INDEXES.items():\n",
    "    if avatar_name not in STOCK_PERSONAL_AVATAR_PROMPTS:\n",
    "        continue\n",
    "    index_path = InputPath(index_path).resolve() / \"**/*.parquet\"\n",
    "    spa_df = pl.scan_parquet(index_path)\n",
    "\n",
    "    text_annotation = STOCK_PERSONAL_AVATAR_PROMPTS.get(avatar_name)\n",
    "    spa_df = spa_df.with_columns(pl.lit(text_annotation).alias(\"actor_free_description\"))\n",
    "\n",
    "\n",
    "    merged_schema = adadeep.schema\n",
    "    merged_schema.update(spa_df.schema)\n",
    "\n",
    "    # 1) Compute the superset of columns\n",
    "    cols1 = set(adadeep.columns)\n",
    "    cols2 = set(spa_df.columns)\n",
    "    all_cols = list(cols1 | cols2)\n",
    "\n",
    "    # 2) Add missing columns with a literal None (or any default) on each side\n",
    "    for col in all_cols:\n",
    "        if col not in adadeep.columns:\n",
    "            adadeep = adadeep.with_columns(pl.lit(None).cast(merged_schema[col]).alias(col))\n",
    "        if col not in spa_df.columns:\n",
    "            spa_df   = spa_df.with_columns(pl.lit(None).cast(merged_schema[col]).alias(col))\n",
    "\n",
    "    spa_df = spa_df.with_columns(pl.lit(\"test\").alias(\"split\"))\n",
    "\n",
    "    # 3) Reorder both DataFrames to the same column order\n",
    "    adadeep = adadeep.select(all_cols)\n",
    "    spa_df = spa_df.select(all_cols)\n",
    "\n",
    "    # 4) Stack them\n",
    "    adadeep = pl.concat([adadeep, spa_df], how=\"diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31382"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adadeep.filter(pl.col(\"split\") == \"train\").select(pl.col(\"synthesia_id\").unique().len()).collect().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435314"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adadeep.filter(pl.col(\"split\") == \"train\").select(pl.len()).collect().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f8fb1298-a5fe-4413-b97c-cc38503bf0d5\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a UUID4\n",
    "random_uuid = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target local path: /scratch/tmp_kaiwenguo/_s3/synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/cc3_v1_adadeep.parquet\n",
      "Ensuring local directory exists: /scratch/tmp_kaiwenguo/_s3/synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5\n",
      "Sinking DataFrame to /scratch/tmp_kaiwenguo/_s3/synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/cc3_v1_adadeep.parquet...\n",
      "Committing /scratch/tmp_kaiwenguo/_s3/synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/cc3_v1_adadeep.parquet to s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/cc3_v1_adadeep.parquet...\n",
      "DataFrame saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from datalib import OutputPath\n",
    "import os\n",
    "\n",
    "output_path_s3 = f\"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_{random_uuid}/cc3_v1_adadeep.parquet\"\n",
    "output_path = OutputPath(output_path_s3)\n",
    "\n",
    "# Resolve to the target local cache path\n",
    "local_file_path = output_path.resolve()\n",
    "print(f\"Target local path: {local_file_path}\")\n",
    "\n",
    "# Get the parent directory of the local path\n",
    "local_dir = os.path.dirname(local_file_path)\n",
    "\n",
    "# Create the local directory, including any necessary parent directories\n",
    "# exist_ok=True prevents an error if the directory already exists\n",
    "print(f\"Ensuring local directory exists: {local_dir}\")\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Now sink the parquet file to the created local directory\n",
    "print(f\"Sinking DataFrame to {local_file_path}...\")\n",
    "adadeep.sink_parquet(\n",
    "    local_file_path,\n",
    "    compression=\"zstd\"\n",
    ") # Execute the sink operation\n",
    "\n",
    "# Commit the result (e.g., upload from local cache to S3)\n",
    "print(f\"Committing {local_file_path} to {output_path_s3}...\")\n",
    "output_path.commit()\n",
    "\n",
    "print(\"DataFrame saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lvm_dataclasses.configs.config_dataclasses:Config: after_crop_hw_ratio is missing, replacing with target_size ratio (only relevant for the crop-to-ratio layer).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index file path: s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5\n",
      "Using DatasetConfig: frames_dataset='video-dataset' video_dataset_sampling_strategy='videos' dataset_layers=['random-reference-frames', 'skeleton-data', 'resize-frames', 'convert-to-rgb-tensor', 'cpu-rendered-dwpose', 'center-crop-frames', 'retry'] n_frames=8 n_reference_frames=1 ref_frames_subsample_rate=1 target_size=[960, 1704] crop_size=[960, 1712] crop_coords=None preferred_data_source=<DataSource.S3: 's3'> mountpoint_base_path=None reference_frames_override=None video_index_file=CloudInputPath('s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5') resolve_paths=False target_fps=30 allow_uneven_frame_step=False ignore_clip_types=None spatial_aug_skip_if_not_train=True spatial_aug_rotation_probability=0.4 spatial_aug_max_rotation_angle=10.0 spatial_aug_max_translation=0.05 spatial_aug_horizontal_flip_probability=0.5 render_worker_id=None use_dwpose_updated_colors=True use_dwpose_lines_on_face=True use_eye_landmarks=False remove_dwpose_hands=False remove_dwpose_face=False dropout_rates=None max_data_retries=10 data_retry_offset=0 verbose_retries=True n_audio_embeddings=None audio_embedding_stride=None audio_embedding_stride_units=None audio_embedding_offset=None max_total_frames=None dataset_clear_cache=True interleave_by_clip_type=False framing='hands-in' framing_probability=None validation_framing=None crop_scaling_factor={'face-closeup': [1.2, 1.5], 'chest-up': [1.2, 1.4], 'waist-up': [1.05, 1.2], 'hands-in': [1.05, 1.2], 'full-body': [1.05, 1.15]} always_crop_per_frame=None css_aligned=None after_crop_hw_ratio=0.5633802816901409 randomise_crop_position=False max_getitems_threads=8 random_bounded_resize_min_size=None random_bounded_resize_max_size=None random_bounded_resize_strict_min_size_guarantee=None frames_hierarchy_config=FramesHierarchyConfig(levels=[HierarchyLevelConfig(buffer_size=33, is_last_latents_conditioned=False, items=[HierarchyItemConfig(original_num_frames=2017, num_latents_in_hierarchy=7, batch_size=1, sampling_weight=1.0, skip_first_latents=False, is_inference_item=True)]), HierarchyLevelConfig(buffer_size=33, is_last_latents_conditioned=True, items=[HierarchyItemConfig(original_num_frames=337, num_latents_in_hierarchy=7, batch_size=1, sampling_weight=1.0, skip_first_latents=False, is_inference_item=True), HierarchyItemConfig(original_num_frames=393, num_latents_in_hierarchy=8, batch_size=1, sampling_weight=1.0, skip_first_latents=True, is_inference_item=False)]), HierarchyLevelConfig(buffer_size=None, is_last_latents_conditioned=True, items=[HierarchyItemConfig(original_num_frames=57, num_latents_in_hierarchy=15, batch_size=1, sampling_weight=1.0, skip_first_latents=False, is_inference_item=True), HierarchyItemConfig(original_num_frames=65, num_latents_in_hierarchy=17, batch_size=1, sampling_weight=1.0, skip_first_latents=True, is_inference_item=False)])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created TRAIN dataset.\n",
      "Number of samples (videos/clips) in train split: 31382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created TEST dataset.\n",
      "Number of samples (videos/clips) in test split: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datalib import InputPath\n",
    "from lvm_dataclasses.configs.config_dataclasses import Phase\n",
    "\n",
    "# Import necessary components from your project\n",
    "from lvm_diffusion_transformer.configs.config_dataclasses import DatasetConfig\n",
    "from lvm_diffusion_transformer.utils.data.dataset import sp_create_dataset\n",
    "\n",
    "index_file_path = InputPath(\"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5\")\n",
    "print(f\"Index file path: {index_file_path}\")\n",
    "\n",
    "# --- 3. Create DatasetConfig ---\n",
    "# Minimal configuration just needs the index file path.\n",
    "# You might need to customize other fields based on your specific needs (frame count, size, etc.)\n",
    "dataset_config = DatasetConfig(video_index_file=index_file_path)\n",
    "print(f\"Using DatasetConfig: {dataset_config}\")\n",
    "\n",
    "# --- 4. Call sp_create_dataset ---\n",
    "# Create the training dataset\n",
    "train_dataset = sp_create_dataset(config=dataset_config, phase=Phase.TRAIN)\n",
    "print(f\"\\nSuccessfully created TRAIN dataset.\")\n",
    "print(f\"Number of samples (videos/clips) in train split: {len(train_dataset)}\")\n",
    "\n",
    "# Example: Accessing the underlying DataFrame (if needed, usually not required)\n",
    "# Note: This gets the filtered DataFrame for the specific phase\n",
    "if hasattr(train_dataset, 'get_frames_dataset'): # Check if layers are applied\n",
    "    train_df = train_dataset.get_frames_dataset().index_dataframe\n",
    "else:\n",
    "    train_df = train_dataset.index_dataframe # Access directly if no layers wrap it\n",
    "\n",
    "# Create the test dataset (if the snapshot contains a test split)\n",
    "test_dataset = sp_create_dataset(config=dataset_config, phase=Phase.TEST)\n",
    "print(f\"Successfully created TEST dataset.\")\n",
    "print(f\"Number of samples (videos/clips) in test split: {len(test_dataset)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
