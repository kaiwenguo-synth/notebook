{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/kaiwenguo/dev/rnd-ditwo/src\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "\n",
    "def from_wan_dit_state_dict(state_dict: dict[str, torch.Tensor], t2v_model: bool = False) -> dict[str, torch.Tensor]:\n",
    "    new_state_dict = {}\n",
    "\n",
    "    weight = state_dict[\"patch_embedding.weight\"]\n",
    "    if t2v_model:\n",
    "        new_shape = list(weight.shape)\n",
    "        new_shape[1] = 36  # Extend the number of channels to 36\n",
    "        new_weight = torch.zeros(new_shape, dtype=weight.dtype, device=weight.device)\n",
    "        new_weight[:, : weight.size(1), ...] = weight\n",
    "        weight = new_weight\n",
    "\n",
    "    weight = weight.moveaxis(-4, -1).flatten(1)\n",
    "    new_state_dict[\"patch_embedder.fc.weight\"] = weight\n",
    "    new_state_dict[\"patch_embedder.fc.bias\"] = state_dict[\"patch_embedding.bias\"]\n",
    "\n",
    "    new_state_dict[\"text_embedder.0.weight\"] = state_dict[\"text_embedding.0.weight\"]\n",
    "    new_state_dict[\"text_embedder.0.bias\"] = state_dict[\"text_embedding.0.bias\"]\n",
    "    new_state_dict[\"text_embedder.2.weight\"] = state_dict[\"text_embedding.2.weight\"]\n",
    "    new_state_dict[\"text_embedder.2.bias\"] = state_dict[\"text_embedding.2.bias\"]\n",
    "\n",
    "    new_state_dict[\"time_embedder.mlp.fc1.weight\"] = state_dict[\"time_embedding.0.weight\"]\n",
    "    new_state_dict[\"time_embedder.mlp.fc1.bias\"] = state_dict[\"time_embedding.0.bias\"]\n",
    "    new_state_dict[\"time_embedder.mlp.fc2.weight\"] = state_dict[\"time_embedding.2.weight\"]\n",
    "    new_state_dict[\"time_embedder.mlp.fc2.bias\"] = state_dict[\"time_embedding.2.bias\"]\n",
    "\n",
    "    # weight = state_dict[\"time_projection.1.weight\"]\n",
    "    # weight = weight.reshape(6, weight.size(1), -1)\n",
    "    # weight = weight[[1, 2, 4, 5, 0, 3], :, :].reshape(6 * weight.size(1), -1)\n",
    "    # new_state_dict[\"time_projection.1.weight\"] = weight\n",
    "    # bias = state_dict[\"time_projection.1.bias\"]\n",
    "    # bias = bias.reshape(6, weight.size(0) // 6)\n",
    "    # bias = bias[[1, 2, 4, 5, 0, 3], :].reshape(-1)\n",
    "    # new_state_dict[\"time_projection.1.bias\"] = bias\n",
    "\n",
    "    weight, bias = state_dict[\"time_projection.1.weight\"], state_dict[\"time_projection.1.bias\"]\n",
    "    weight, bias = weight.reshape(6, weight.size(0) // 6, -1), bias.reshape(6, weight.size(0) // 6)\n",
    "    new_state_dict[\"time_projection.1.weight\"] = weight\n",
    "    new_state_dict[\"time_projection.1.bias\"] = bias\n",
    "\n",
    "    num_transformer_blocks = len(\n",
    "        set(\n",
    "            match.group(1)\n",
    "            for match in (re.match(r\"blocks\\.(\\d+)\", key) for key in state_dict.keys())\n",
    "            if match is not None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(num_transformer_blocks):\n",
    "        weight = state_dict[f\"blocks.{i}.self_attn.norm_q.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.q_norm.weight\"] = weight * np.sqrt(weight.size(-1))\n",
    "        weight = state_dict[f\"blocks.{i}.self_attn.norm_k.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.k_norm.weight\"] = weight * np.sqrt(weight.size(-1))\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_q.weight\"] = state_dict[f\"blocks.{i}.self_attn.q.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_q.bias\"] = state_dict[f\"blocks.{i}.self_attn.q.bias\"]\n",
    "        # new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_q.bias\"] = (\n",
    "        #     state_dict[f\"blocks.{i}.self_attn.q.bias\"]\n",
    "        #     + state_dict[f\"blocks.{i}.self_attn.q.weight\"] @ state_dict[f\"blocks.{i}.modulation\"][0, 0, :]\n",
    "        # )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_k.weight\"] = state_dict[f\"blocks.{i}.self_attn.k.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_k.bias\"] = state_dict[f\"blocks.{i}.self_attn.k.bias\"]\n",
    "        # new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_k.bias\"] = (\n",
    "        #     state_dict[f\"blocks.{i}.self_attn.k.bias\"]\n",
    "        #     + state_dict[f\"blocks.{i}.self_attn.k.weight\"] @ state_dict[f\"blocks.{i}.modulation\"][0, 0, :]\n",
    "        # )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_v.weight\"] = state_dict[f\"blocks.{i}.self_attn.v.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_v.bias\"] = state_dict[f\"blocks.{i}.self_attn.v.bias\"]\n",
    "        # new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_v.bias\"] = (\n",
    "        #     state_dict[f\"blocks.{i}.self_attn.v.bias\"]\n",
    "        #     + state_dict[f\"blocks.{i}.self_attn.v.weight\"] @ state_dict[f\"blocks.{i}.modulation\"][0, 0, :]\n",
    "        # )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_out.weight\"] = state_dict[f\"blocks.{i}.self_attn.o.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.self_attn.fc_out.bias\"] = state_dict[f\"blocks.{i}.self_attn.o.bias\"]\n",
    "        weight = state_dict[f\"blocks.{i}.cross_attn.norm_q.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.q_norm.weight\"] = weight * np.sqrt(weight.size(-1))\n",
    "        weight = state_dict[f\"blocks.{i}.cross_attn.norm_k.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.k_norm.weight\"] = weight * np.sqrt(weight.size(-1))\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_q.weight\"] = (\n",
    "            state_dict[f\"blocks.{i}.cross_attn.q.weight\"] * state_dict[f\"blocks.{i}.norm3.weight\"][None, :]\n",
    "        )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_q.bias\"] = (\n",
    "            state_dict[f\"blocks.{i}.cross_attn.q.bias\"]\n",
    "            + state_dict[f\"blocks.{i}.cross_attn.q.weight\"] @ state_dict[f\"blocks.{i}.norm3.bias\"]\n",
    "        )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_k.weight\"] = state_dict[f\"blocks.{i}.cross_attn.k.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_k.bias\"] = state_dict[f\"blocks.{i}.cross_attn.k.bias\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_v.weight\"] = state_dict[f\"blocks.{i}.cross_attn.v.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_v.bias\"] = state_dict[f\"blocks.{i}.cross_attn.v.bias\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_out.weight\"] = state_dict[\n",
    "            f\"blocks.{i}.cross_attn.o.weight\"\n",
    "        ]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.cross_attn.fc_out.bias\"] = state_dict[f\"blocks.{i}.cross_attn.o.bias\"]\n",
    "\n",
    "        new_state_dict[f\"transformer_blocks.{i}.ffn.0.weight\"] = state_dict[f\"blocks.{i}.ffn.0.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.ffn.0.bias\"] = state_dict[f\"blocks.{i}.ffn.0.bias\"]\n",
    "        # new_state_dict[f\"transformer_blocks.{i}.ffn.0.bias\"] = (\n",
    "        #     state_dict[f\"blocks.{i}.ffn.0.bias\"]\n",
    "        #     + state_dict[f\"blocks.{i}.ffn.0.weight\"] @ state_dict[f\"blocks.{i}.modulation\"][0, 3, :]\n",
    "        # )\n",
    "        new_state_dict[f\"transformer_blocks.{i}.ffn.2.weight\"] = state_dict[f\"blocks.{i}.ffn.2.weight\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.ffn.2.bias\"] = state_dict[f\"blocks.{i}.ffn.2.bias\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.vision_time_modulation_bias\"] = state_dict[f\"blocks.{i}.modulation\"]\n",
    "        new_state_dict[f\"transformer_blocks.{i}.reference_time_modulation_bias\"] = torch.zeros_like(\n",
    "            state_dict[f\"blocks.{i}.modulation\"],\n",
    "            dtype=state_dict[f\"blocks.{i}.modulation\"].dtype,\n",
    "            device=state_dict[f\"blocks.{i}.modulation\"].device,\n",
    "        )\n",
    "        # new_state_dict[f\"transformer_blocks.{i}.time_modulation_bias\"] = state_dict[f\"blocks.{i}.modulation\"][\n",
    "        #     :,\n",
    "        #     [1, 2, 4, 5],\n",
    "        #     :,\n",
    "        # ]\n",
    "\n",
    "    new_state_dict[\"final_layer.fc.weight\"] = state_dict[\"head.head.weight\"]\n",
    "    new_state_dict[\"final_layer.fc.bias\"] = (\n",
    "        state_dict[\"head.head.bias\"] + state_dict[\"head.head.weight\"] @ state_dict[\"head.modulation\"][0, 0]\n",
    "    )\n",
    "    new_state_dict[\"final_layer.time_modulation_bias\"] = state_dict[\"head.modulation\"][:, 1:]\n",
    "\n",
    "    return new_state_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert WAN-FAN with `reference_time_modulation_bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.cross_attn.k.bias\n",
      "blocks.0.cross_attn.k.weight\n",
      "blocks.0.cross_attn.k_img.bias\n",
      "blocks.0.cross_attn.k_img.weight\n",
      "blocks.0.cross_attn.norm_k.weight\n",
      "blocks.0.cross_attn.norm_k_img.weight\n",
      "blocks.0.cross_attn.norm_q.weight\n",
      "blocks.0.cross_attn.o.bias\n",
      "blocks.0.cross_attn.o.weight\n",
      "blocks.0.cross_attn.q.bias\n",
      "blocks.0.cross_attn.q.weight\n",
      "blocks.0.cross_attn.v.bias\n",
      "blocks.0.cross_attn.v.weight\n",
      "blocks.0.cross_attn.v_img.bias\n",
      "blocks.0.cross_attn.v_img.weight\n",
      "blocks.0.ffn.0.bias\n",
      "blocks.0.ffn.0.weight\n",
      "blocks.0.ffn.2.bias\n",
      "blocks.0.ffn.2.weight\n",
      "blocks.0.modulation\n",
      "blocks.0.norm3.bias\n",
      "blocks.0.norm3.weight\n",
      "blocks.0.self_attn.k.bias\n",
      "blocks.0.self_attn.k.weight\n",
      "blocks.0.self_attn.norm_k.weight\n",
      "blocks.0.self_attn.norm_q.weight\n",
      "blocks.0.self_attn.o.bias\n",
      "blocks.0.self_attn.o.weight\n",
      "blocks.0.self_attn.q.bias\n",
      "blocks.0.self_attn.q.weight\n",
      "blocks.0.self_attn.v.bias\n",
      "blocks.0.self_attn.v.weight\n",
      "blocks.1.cross_attn.k.bias\n",
      "blocks.1.cross_attn.k.weight\n",
      "blocks.1.cross_attn.k_img.bias\n",
      "blocks.1.cross_attn.k_img.weight\n",
      "blocks.1.cross_attn.norm_k.weight\n",
      "blocks.1.cross_attn.norm_k_img.weight\n",
      "blocks.1.cross_attn.norm_q.weight\n",
      "blocks.1.cross_attn.o.bias\n",
      "blocks.1.cross_attn.o.weight\n",
      "blocks.1.cross_attn.q.bias\n",
      "blocks.1.cross_attn.q.weight\n",
      "blocks.1.cross_attn.v.bias\n",
      "blocks.1.cross_attn.v.weight\n",
      "blocks.1.cross_attn.v_img.bias\n",
      "blocks.1.cross_attn.v_img.weight\n",
      "blocks.1.ffn.0.bias\n",
      "blocks.1.ffn.0.weight\n",
      "blocks.1.ffn.2.bias\n",
      "blocks.1.ffn.2.weight\n",
      "blocks.1.modulation\n",
      "blocks.1.norm3.bias\n",
      "blocks.1.norm3.weight\n",
      "blocks.1.self_attn.k.bias\n",
      "blocks.1.self_attn.k.weight\n",
      "blocks.1.self_attn.norm_k.weight\n",
      "blocks.1.self_attn.norm_q.weight\n",
      "blocks.1.self_attn.o.bias\n",
      "blocks.1.self_attn.o.weight\n",
      "blocks.1.self_attn.q.bias\n",
      "blocks.1.self_attn.q.weight\n",
      "blocks.1.self_attn.v.bias\n",
      "blocks.1.self_attn.v.weight\n",
      "blocks.10.cross_attn.k.bias\n",
      "blocks.10.cross_attn.k.weight\n",
      "blocks.10.cross_attn.k_img.bias\n",
      "blocks.10.cross_attn.k_img.weight\n",
      "blocks.10.cross_attn.norm_k.weight\n",
      "blocks.10.cross_attn.norm_k_img.weight\n",
      "blocks.10.cross_attn.norm_q.weight\n",
      "blocks.10.cross_attn.o.bias\n",
      "blocks.10.cross_attn.o.weight\n",
      "blocks.10.cross_attn.q.bias\n",
      "blocks.10.cross_attn.q.weight\n",
      "blocks.10.cross_attn.v.bias\n",
      "blocks.10.cross_attn.v.weight\n",
      "blocks.10.cross_attn.v_img.bias\n",
      "blocks.10.cross_attn.v_img.weight\n",
      "blocks.10.ffn.0.bias\n",
      "blocks.10.ffn.0.weight\n",
      "blocks.10.ffn.2.bias\n",
      "blocks.10.ffn.2.weight\n",
      "blocks.10.modulation\n",
      "blocks.10.norm3.bias\n",
      "blocks.10.norm3.weight\n",
      "blocks.10.self_attn.k.bias\n",
      "blocks.10.self_attn.k.weight\n",
      "blocks.10.self_attn.norm_k.weight\n",
      "blocks.10.self_attn.norm_q.weight\n",
      "blocks.10.self_attn.o.bias\n",
      "blocks.10.self_attn.o.weight\n",
      "blocks.10.self_attn.q.bias\n",
      "blocks.10.self_attn.q.weight\n",
      "blocks.10.self_attn.v.bias\n",
      "blocks.10.self_attn.v.weight\n",
      "blocks.11.cross_attn.k.bias\n",
      "blocks.11.cross_attn.k.weight\n",
      "blocks.11.cross_attn.k_img.bias\n",
      "blocks.11.cross_attn.k_img.weight\n",
      "blocks.11.cross_attn.norm_k.weight\n",
      "blocks.11.cross_attn.norm_k_img.weight\n",
      "blocks.11.cross_attn.norm_q.weight\n",
      "blocks.11.cross_attn.o.bias\n",
      "blocks.11.cross_attn.o.weight\n",
      "blocks.11.cross_attn.q.bias\n",
      "blocks.11.cross_attn.q.weight\n",
      "blocks.11.cross_attn.v.bias\n",
      "blocks.11.cross_attn.v.weight\n",
      "blocks.11.cross_attn.v_img.bias\n",
      "blocks.11.cross_attn.v_img.weight\n",
      "blocks.11.ffn.0.bias\n",
      "blocks.11.ffn.0.weight\n",
      "blocks.11.ffn.2.bias\n",
      "blocks.11.ffn.2.weight\n",
      "blocks.11.modulation\n",
      "blocks.11.norm3.bias\n",
      "blocks.11.norm3.weight\n",
      "blocks.11.self_attn.k.bias\n",
      "blocks.11.self_attn.k.weight\n",
      "blocks.11.self_attn.norm_k.weight\n",
      "blocks.11.self_attn.norm_q.weight\n",
      "blocks.11.self_attn.o.bias\n",
      "blocks.11.self_attn.o.weight\n",
      "blocks.11.self_attn.q.bias\n",
      "blocks.11.self_attn.q.weight\n",
      "blocks.11.self_attn.v.bias\n",
      "blocks.11.self_attn.v.weight\n",
      "blocks.12.cross_attn.k.bias\n",
      "blocks.12.cross_attn.k.weight\n",
      "blocks.12.cross_attn.k_img.bias\n",
      "blocks.12.cross_attn.k_img.weight\n",
      "blocks.12.cross_attn.norm_k.weight\n",
      "blocks.12.cross_attn.norm_k_img.weight\n",
      "blocks.12.cross_attn.norm_q.weight\n",
      "blocks.12.cross_attn.o.bias\n",
      "blocks.12.cross_attn.o.weight\n",
      "blocks.12.cross_attn.q.bias\n",
      "blocks.12.cross_attn.q.weight\n",
      "blocks.12.cross_attn.v.bias\n",
      "blocks.12.cross_attn.v.weight\n",
      "blocks.12.cross_attn.v_img.bias\n",
      "blocks.12.cross_attn.v_img.weight\n",
      "blocks.12.ffn.0.bias\n",
      "blocks.12.ffn.0.weight\n",
      "blocks.12.ffn.2.bias\n",
      "blocks.12.ffn.2.weight\n",
      "blocks.12.modulation\n",
      "blocks.12.norm3.bias\n",
      "blocks.12.norm3.weight\n",
      "blocks.12.self_attn.k.bias\n",
      "blocks.12.self_attn.k.weight\n",
      "blocks.12.self_attn.norm_k.weight\n",
      "blocks.12.self_attn.norm_q.weight\n",
      "blocks.12.self_attn.o.bias\n",
      "blocks.12.self_attn.o.weight\n",
      "blocks.12.self_attn.q.bias\n",
      "blocks.12.self_attn.q.weight\n",
      "blocks.12.self_attn.v.bias\n",
      "blocks.12.self_attn.v.weight\n",
      "blocks.13.cross_attn.k.bias\n",
      "blocks.13.cross_attn.k.weight\n",
      "blocks.13.cross_attn.k_img.bias\n",
      "blocks.13.cross_attn.k_img.weight\n",
      "blocks.13.cross_attn.norm_k.weight\n",
      "blocks.13.cross_attn.norm_k_img.weight\n",
      "blocks.13.cross_attn.norm_q.weight\n",
      "blocks.13.cross_attn.o.bias\n",
      "blocks.13.cross_attn.o.weight\n",
      "blocks.13.cross_attn.q.bias\n",
      "blocks.13.cross_attn.q.weight\n",
      "blocks.13.cross_attn.v.bias\n",
      "blocks.13.cross_attn.v.weight\n",
      "blocks.13.cross_attn.v_img.bias\n",
      "blocks.13.cross_attn.v_img.weight\n",
      "blocks.13.ffn.0.bias\n",
      "blocks.13.ffn.0.weight\n",
      "blocks.13.ffn.2.bias\n",
      "blocks.13.ffn.2.weight\n",
      "blocks.13.modulation\n",
      "blocks.13.norm3.bias\n",
      "blocks.13.norm3.weight\n",
      "blocks.13.self_attn.k.bias\n",
      "blocks.13.self_attn.k.weight\n",
      "blocks.13.self_attn.norm_k.weight\n",
      "blocks.13.self_attn.norm_q.weight\n",
      "blocks.13.self_attn.o.bias\n",
      "blocks.13.self_attn.o.weight\n",
      "blocks.13.self_attn.q.bias\n",
      "blocks.13.self_attn.q.weight\n",
      "blocks.13.self_attn.v.bias\n",
      "blocks.13.self_attn.v.weight\n",
      "blocks.14.cross_attn.k.bias\n",
      "blocks.14.cross_attn.k.weight\n",
      "blocks.14.cross_attn.k_img.bias\n",
      "blocks.14.cross_attn.k_img.weight\n",
      "blocks.14.cross_attn.norm_k.weight\n",
      "blocks.14.cross_attn.norm_k_img.weight\n",
      "blocks.14.cross_attn.norm_q.weight\n",
      "blocks.14.cross_attn.o.bias\n",
      "blocks.14.cross_attn.o.weight\n",
      "blocks.14.cross_attn.q.bias\n",
      "blocks.14.cross_attn.q.weight\n",
      "blocks.14.cross_attn.v.bias\n",
      "blocks.14.cross_attn.v.weight\n",
      "blocks.14.cross_attn.v_img.bias\n",
      "blocks.14.cross_attn.v_img.weight\n",
      "blocks.14.ffn.0.bias\n",
      "blocks.14.ffn.0.weight\n",
      "blocks.14.ffn.2.bias\n",
      "blocks.14.ffn.2.weight\n",
      "blocks.14.modulation\n",
      "blocks.14.norm3.bias\n",
      "blocks.14.norm3.weight\n",
      "blocks.14.self_attn.k.bias\n",
      "blocks.14.self_attn.k.weight\n",
      "blocks.14.self_attn.norm_k.weight\n",
      "blocks.14.self_attn.norm_q.weight\n",
      "blocks.14.self_attn.o.bias\n",
      "blocks.14.self_attn.o.weight\n",
      "blocks.14.self_attn.q.bias\n",
      "blocks.14.self_attn.q.weight\n",
      "blocks.14.self_attn.v.bias\n",
      "blocks.14.self_attn.v.weight\n",
      "blocks.15.cross_attn.k.bias\n",
      "blocks.15.cross_attn.k.weight\n",
      "blocks.15.cross_attn.k_img.bias\n",
      "blocks.15.cross_attn.k_img.weight\n",
      "blocks.15.cross_attn.norm_k.weight\n",
      "blocks.15.cross_attn.norm_k_img.weight\n",
      "blocks.15.cross_attn.norm_q.weight\n",
      "blocks.15.cross_attn.o.bias\n",
      "blocks.15.cross_attn.o.weight\n",
      "blocks.15.cross_attn.q.bias\n",
      "blocks.15.cross_attn.q.weight\n",
      "blocks.15.cross_attn.v.bias\n",
      "blocks.15.cross_attn.v.weight\n",
      "blocks.15.cross_attn.v_img.bias\n",
      "blocks.15.cross_attn.v_img.weight\n",
      "blocks.15.ffn.0.bias\n",
      "blocks.15.ffn.0.weight\n",
      "blocks.15.ffn.2.bias\n",
      "blocks.15.ffn.2.weight\n",
      "blocks.15.modulation\n",
      "blocks.15.norm3.bias\n",
      "blocks.15.norm3.weight\n",
      "blocks.15.self_attn.k.bias\n",
      "blocks.15.self_attn.k.weight\n",
      "blocks.15.self_attn.norm_k.weight\n",
      "blocks.15.self_attn.norm_q.weight\n",
      "blocks.15.self_attn.o.bias\n",
      "blocks.15.self_attn.o.weight\n",
      "blocks.15.self_attn.q.bias\n",
      "blocks.15.self_attn.q.weight\n",
      "blocks.15.self_attn.v.bias\n",
      "blocks.15.self_attn.v.weight\n",
      "blocks.16.cross_attn.k.bias\n",
      "blocks.16.cross_attn.k.weight\n",
      "blocks.16.cross_attn.k_img.bias\n",
      "blocks.16.cross_attn.k_img.weight\n",
      "blocks.16.cross_attn.norm_k.weight\n",
      "blocks.16.cross_attn.norm_k_img.weight\n",
      "blocks.16.cross_attn.norm_q.weight\n",
      "blocks.16.cross_attn.o.bias\n",
      "blocks.16.cross_attn.o.weight\n",
      "blocks.16.cross_attn.q.bias\n",
      "blocks.16.cross_attn.q.weight\n",
      "blocks.16.cross_attn.v.bias\n",
      "blocks.16.cross_attn.v.weight\n",
      "blocks.16.cross_attn.v_img.bias\n",
      "blocks.16.cross_attn.v_img.weight\n",
      "blocks.16.ffn.0.bias\n",
      "blocks.16.ffn.0.weight\n",
      "blocks.16.ffn.2.bias\n",
      "blocks.16.ffn.2.weight\n",
      "blocks.16.modulation\n",
      "blocks.16.norm3.bias\n",
      "blocks.16.norm3.weight\n",
      "blocks.16.self_attn.k.bias\n",
      "blocks.16.self_attn.k.weight\n",
      "blocks.16.self_attn.norm_k.weight\n",
      "blocks.16.self_attn.norm_q.weight\n",
      "blocks.16.self_attn.o.bias\n",
      "blocks.16.self_attn.o.weight\n",
      "blocks.16.self_attn.q.bias\n",
      "blocks.16.self_attn.q.weight\n",
      "blocks.16.self_attn.v.bias\n",
      "blocks.16.self_attn.v.weight\n",
      "blocks.17.cross_attn.k.bias\n",
      "blocks.17.cross_attn.k.weight\n",
      "blocks.17.cross_attn.k_img.bias\n",
      "blocks.17.cross_attn.k_img.weight\n",
      "blocks.17.cross_attn.norm_k.weight\n",
      "blocks.17.cross_attn.norm_k_img.weight\n",
      "blocks.17.cross_attn.norm_q.weight\n",
      "blocks.17.cross_attn.o.bias\n",
      "blocks.17.cross_attn.o.weight\n",
      "blocks.17.cross_attn.q.bias\n",
      "blocks.17.cross_attn.q.weight\n",
      "blocks.17.cross_attn.v.bias\n",
      "blocks.17.cross_attn.v.weight\n",
      "blocks.17.cross_attn.v_img.bias\n",
      "blocks.17.cross_attn.v_img.weight\n",
      "blocks.17.ffn.0.bias\n",
      "blocks.17.ffn.0.weight\n",
      "blocks.17.ffn.2.bias\n",
      "blocks.17.ffn.2.weight\n",
      "blocks.17.modulation\n",
      "blocks.17.norm3.bias\n",
      "blocks.17.norm3.weight\n",
      "blocks.17.self_attn.k.bias\n",
      "blocks.17.self_attn.k.weight\n",
      "blocks.17.self_attn.norm_k.weight\n",
      "blocks.17.self_attn.norm_q.weight\n",
      "blocks.17.self_attn.o.bias\n",
      "blocks.17.self_attn.o.weight\n",
      "blocks.17.self_attn.q.bias\n",
      "blocks.17.self_attn.q.weight\n",
      "blocks.17.self_attn.v.bias\n",
      "blocks.17.self_attn.v.weight\n",
      "blocks.18.cross_attn.k.bias\n",
      "blocks.18.cross_attn.k.weight\n",
      "blocks.18.cross_attn.k_img.bias\n",
      "blocks.18.cross_attn.k_img.weight\n",
      "blocks.18.cross_attn.norm_k.weight\n",
      "blocks.18.cross_attn.norm_k_img.weight\n",
      "blocks.18.cross_attn.norm_q.weight\n",
      "blocks.18.cross_attn.o.bias\n",
      "blocks.18.cross_attn.o.weight\n",
      "blocks.18.cross_attn.q.bias\n",
      "blocks.18.cross_attn.q.weight\n",
      "blocks.18.cross_attn.v.bias\n",
      "blocks.18.cross_attn.v.weight\n",
      "blocks.18.cross_attn.v_img.bias\n",
      "blocks.18.cross_attn.v_img.weight\n",
      "blocks.18.ffn.0.bias\n",
      "blocks.18.ffn.0.weight\n",
      "blocks.18.ffn.2.bias\n",
      "blocks.18.ffn.2.weight\n",
      "blocks.18.modulation\n",
      "blocks.18.norm3.bias\n",
      "blocks.18.norm3.weight\n",
      "blocks.18.self_attn.k.bias\n",
      "blocks.18.self_attn.k.weight\n",
      "blocks.18.self_attn.norm_k.weight\n",
      "blocks.18.self_attn.norm_q.weight\n",
      "blocks.18.self_attn.o.bias\n",
      "blocks.18.self_attn.o.weight\n",
      "blocks.18.self_attn.q.bias\n",
      "blocks.18.self_attn.q.weight\n",
      "blocks.18.self_attn.v.bias\n",
      "blocks.18.self_attn.v.weight\n",
      "blocks.19.cross_attn.k.bias\n",
      "blocks.19.cross_attn.k.weight\n",
      "blocks.19.cross_attn.k_img.bias\n",
      "blocks.19.cross_attn.k_img.weight\n",
      "blocks.19.cross_attn.norm_k.weight\n",
      "blocks.19.cross_attn.norm_k_img.weight\n",
      "blocks.19.cross_attn.norm_q.weight\n",
      "blocks.19.cross_attn.o.bias\n",
      "blocks.19.cross_attn.o.weight\n",
      "blocks.19.cross_attn.q.bias\n",
      "blocks.19.cross_attn.q.weight\n",
      "blocks.19.cross_attn.v.bias\n",
      "blocks.19.cross_attn.v.weight\n",
      "blocks.19.cross_attn.v_img.bias\n",
      "blocks.19.cross_attn.v_img.weight\n",
      "blocks.19.ffn.0.bias\n",
      "blocks.19.ffn.0.weight\n",
      "blocks.19.ffn.2.bias\n",
      "blocks.19.ffn.2.weight\n",
      "blocks.19.modulation\n",
      "blocks.19.norm3.bias\n",
      "blocks.19.norm3.weight\n",
      "blocks.19.self_attn.k.bias\n",
      "blocks.19.self_attn.k.weight\n",
      "blocks.19.self_attn.norm_k.weight\n",
      "blocks.19.self_attn.norm_q.weight\n",
      "blocks.19.self_attn.o.bias\n",
      "blocks.19.self_attn.o.weight\n",
      "blocks.19.self_attn.q.bias\n",
      "blocks.19.self_attn.q.weight\n",
      "blocks.19.self_attn.v.bias\n",
      "blocks.19.self_attn.v.weight\n",
      "blocks.2.cross_attn.k.bias\n",
      "blocks.2.cross_attn.k.weight\n",
      "blocks.2.cross_attn.k_img.bias\n",
      "blocks.2.cross_attn.k_img.weight\n",
      "blocks.2.cross_attn.norm_k.weight\n",
      "blocks.2.cross_attn.norm_k_img.weight\n",
      "blocks.2.cross_attn.norm_q.weight\n",
      "blocks.2.cross_attn.o.bias\n",
      "blocks.2.cross_attn.o.weight\n",
      "blocks.2.cross_attn.q.bias\n",
      "blocks.2.cross_attn.q.weight\n",
      "blocks.2.cross_attn.v.bias\n",
      "blocks.2.cross_attn.v.weight\n",
      "blocks.2.cross_attn.v_img.bias\n",
      "blocks.2.cross_attn.v_img.weight\n",
      "blocks.2.ffn.0.bias\n",
      "blocks.2.ffn.0.weight\n",
      "blocks.2.ffn.2.bias\n",
      "blocks.2.ffn.2.weight\n",
      "blocks.2.modulation\n",
      "blocks.2.norm3.bias\n",
      "blocks.2.norm3.weight\n",
      "blocks.2.self_attn.k.bias\n",
      "blocks.2.self_attn.k.weight\n",
      "blocks.2.self_attn.norm_k.weight\n",
      "blocks.2.self_attn.norm_q.weight\n",
      "blocks.2.self_attn.o.bias\n",
      "blocks.2.self_attn.o.weight\n",
      "blocks.2.self_attn.q.bias\n",
      "blocks.2.self_attn.q.weight\n",
      "blocks.2.self_attn.v.bias\n",
      "blocks.2.self_attn.v.weight\n",
      "blocks.20.cross_attn.k.bias\n",
      "blocks.20.cross_attn.k.weight\n",
      "blocks.20.cross_attn.k_img.bias\n",
      "blocks.20.cross_attn.k_img.weight\n",
      "blocks.20.cross_attn.norm_k.weight\n",
      "blocks.20.cross_attn.norm_k_img.weight\n",
      "blocks.20.cross_attn.norm_q.weight\n",
      "blocks.20.cross_attn.o.bias\n",
      "blocks.20.cross_attn.o.weight\n",
      "blocks.20.cross_attn.q.bias\n",
      "blocks.20.cross_attn.q.weight\n",
      "blocks.20.cross_attn.v.bias\n",
      "blocks.20.cross_attn.v.weight\n",
      "blocks.20.cross_attn.v_img.bias\n",
      "blocks.20.cross_attn.v_img.weight\n",
      "blocks.20.ffn.0.bias\n",
      "blocks.20.ffn.0.weight\n",
      "blocks.20.ffn.2.bias\n",
      "blocks.20.ffn.2.weight\n",
      "blocks.20.modulation\n",
      "blocks.20.norm3.bias\n",
      "blocks.20.norm3.weight\n",
      "blocks.20.self_attn.k.bias\n",
      "blocks.20.self_attn.k.weight\n",
      "blocks.20.self_attn.norm_k.weight\n",
      "blocks.20.self_attn.norm_q.weight\n",
      "blocks.20.self_attn.o.bias\n",
      "blocks.20.self_attn.o.weight\n",
      "blocks.20.self_attn.q.bias\n",
      "blocks.20.self_attn.q.weight\n",
      "blocks.20.self_attn.v.bias\n",
      "blocks.20.self_attn.v.weight\n",
      "blocks.21.cross_attn.k.bias\n",
      "blocks.21.cross_attn.k.weight\n",
      "blocks.21.cross_attn.k_img.bias\n",
      "blocks.21.cross_attn.k_img.weight\n",
      "blocks.21.cross_attn.norm_k.weight\n",
      "blocks.21.cross_attn.norm_k_img.weight\n",
      "blocks.21.cross_attn.norm_q.weight\n",
      "blocks.21.cross_attn.o.bias\n",
      "blocks.21.cross_attn.o.weight\n",
      "blocks.21.cross_attn.q.bias\n",
      "blocks.21.cross_attn.q.weight\n",
      "blocks.21.cross_attn.v.bias\n",
      "blocks.21.cross_attn.v.weight\n",
      "blocks.21.cross_attn.v_img.bias\n",
      "blocks.21.cross_attn.v_img.weight\n",
      "blocks.21.ffn.0.bias\n",
      "blocks.21.ffn.0.weight\n",
      "blocks.21.ffn.2.bias\n",
      "blocks.21.ffn.2.weight\n",
      "blocks.21.modulation\n",
      "blocks.21.norm3.bias\n",
      "blocks.21.norm3.weight\n",
      "blocks.21.self_attn.k.bias\n",
      "blocks.21.self_attn.k.weight\n",
      "blocks.21.self_attn.norm_k.weight\n",
      "blocks.21.self_attn.norm_q.weight\n",
      "blocks.21.self_attn.o.bias\n",
      "blocks.21.self_attn.o.weight\n",
      "blocks.21.self_attn.q.bias\n",
      "blocks.21.self_attn.q.weight\n",
      "blocks.21.self_attn.v.bias\n",
      "blocks.21.self_attn.v.weight\n",
      "blocks.22.cross_attn.k.bias\n",
      "blocks.22.cross_attn.k.weight\n",
      "blocks.22.cross_attn.k_img.bias\n",
      "blocks.22.cross_attn.k_img.weight\n",
      "blocks.22.cross_attn.norm_k.weight\n",
      "blocks.22.cross_attn.norm_k_img.weight\n",
      "blocks.22.cross_attn.norm_q.weight\n",
      "blocks.22.cross_attn.o.bias\n",
      "blocks.22.cross_attn.o.weight\n",
      "blocks.22.cross_attn.q.bias\n",
      "blocks.22.cross_attn.q.weight\n",
      "blocks.22.cross_attn.v.bias\n",
      "blocks.22.cross_attn.v.weight\n",
      "blocks.22.cross_attn.v_img.bias\n",
      "blocks.22.cross_attn.v_img.weight\n",
      "blocks.22.ffn.0.bias\n",
      "blocks.22.ffn.0.weight\n",
      "blocks.22.ffn.2.bias\n",
      "blocks.22.ffn.2.weight\n",
      "blocks.22.modulation\n",
      "blocks.22.norm3.bias\n",
      "blocks.22.norm3.weight\n",
      "blocks.22.self_attn.k.bias\n",
      "blocks.22.self_attn.k.weight\n",
      "blocks.22.self_attn.norm_k.weight\n",
      "blocks.22.self_attn.norm_q.weight\n",
      "blocks.22.self_attn.o.bias\n",
      "blocks.22.self_attn.o.weight\n",
      "blocks.22.self_attn.q.bias\n",
      "blocks.22.self_attn.q.weight\n",
      "blocks.22.self_attn.v.bias\n",
      "blocks.22.self_attn.v.weight\n",
      "blocks.23.cross_attn.k.bias\n",
      "blocks.23.cross_attn.k.weight\n",
      "blocks.23.cross_attn.k_img.bias\n",
      "blocks.23.cross_attn.k_img.weight\n",
      "blocks.23.cross_attn.norm_k.weight\n",
      "blocks.23.cross_attn.norm_k_img.weight\n",
      "blocks.23.cross_attn.norm_q.weight\n",
      "blocks.23.cross_attn.o.bias\n",
      "blocks.23.cross_attn.o.weight\n",
      "blocks.23.cross_attn.q.bias\n",
      "blocks.23.cross_attn.q.weight\n",
      "blocks.23.cross_attn.v.bias\n",
      "blocks.23.cross_attn.v.weight\n",
      "blocks.23.cross_attn.v_img.bias\n",
      "blocks.23.cross_attn.v_img.weight\n",
      "blocks.23.ffn.0.bias\n",
      "blocks.23.ffn.0.weight\n",
      "blocks.23.ffn.2.bias\n",
      "blocks.23.ffn.2.weight\n",
      "blocks.23.modulation\n",
      "blocks.23.norm3.bias\n",
      "blocks.23.norm3.weight\n",
      "blocks.23.self_attn.k.bias\n",
      "blocks.23.self_attn.k.weight\n",
      "blocks.23.self_attn.norm_k.weight\n",
      "blocks.23.self_attn.norm_q.weight\n",
      "blocks.23.self_attn.o.bias\n",
      "blocks.23.self_attn.o.weight\n",
      "blocks.23.self_attn.q.bias\n",
      "blocks.23.self_attn.q.weight\n",
      "blocks.23.self_attn.v.bias\n",
      "blocks.23.self_attn.v.weight\n",
      "blocks.24.cross_attn.k.bias\n",
      "blocks.24.cross_attn.k.weight\n",
      "blocks.24.cross_attn.k_img.bias\n",
      "blocks.24.cross_attn.k_img.weight\n",
      "blocks.24.cross_attn.norm_k.weight\n",
      "blocks.24.cross_attn.norm_k_img.weight\n",
      "blocks.24.cross_attn.norm_q.weight\n",
      "blocks.24.cross_attn.o.bias\n",
      "blocks.24.cross_attn.o.weight\n",
      "blocks.24.cross_attn.q.bias\n",
      "blocks.24.cross_attn.q.weight\n",
      "blocks.24.cross_attn.v.bias\n",
      "blocks.24.cross_attn.v.weight\n",
      "blocks.24.cross_attn.v_img.bias\n",
      "blocks.24.cross_attn.v_img.weight\n",
      "blocks.24.ffn.0.bias\n",
      "blocks.24.ffn.0.weight\n",
      "blocks.24.ffn.2.bias\n",
      "blocks.24.ffn.2.weight\n",
      "blocks.24.modulation\n",
      "blocks.24.norm3.bias\n",
      "blocks.24.norm3.weight\n",
      "blocks.24.self_attn.k.bias\n",
      "blocks.24.self_attn.k.weight\n",
      "blocks.24.self_attn.norm_k.weight\n",
      "blocks.24.self_attn.norm_q.weight\n",
      "blocks.24.self_attn.o.bias\n",
      "blocks.24.self_attn.o.weight\n",
      "blocks.24.self_attn.q.bias\n",
      "blocks.24.self_attn.q.weight\n",
      "blocks.24.self_attn.v.bias\n",
      "blocks.24.self_attn.v.weight\n",
      "blocks.25.cross_attn.k.bias\n",
      "blocks.25.cross_attn.k.weight\n",
      "blocks.25.cross_attn.k_img.bias\n",
      "blocks.25.cross_attn.k_img.weight\n",
      "blocks.25.cross_attn.norm_k.weight\n",
      "blocks.25.cross_attn.norm_k_img.weight\n",
      "blocks.25.cross_attn.norm_q.weight\n",
      "blocks.25.cross_attn.o.bias\n",
      "blocks.25.cross_attn.o.weight\n",
      "blocks.25.cross_attn.q.bias\n",
      "blocks.25.cross_attn.q.weight\n",
      "blocks.25.cross_attn.v.bias\n",
      "blocks.25.cross_attn.v.weight\n",
      "blocks.25.cross_attn.v_img.bias\n",
      "blocks.25.cross_attn.v_img.weight\n",
      "blocks.25.ffn.0.bias\n",
      "blocks.25.ffn.0.weight\n",
      "blocks.25.ffn.2.bias\n",
      "blocks.25.ffn.2.weight\n",
      "blocks.25.modulation\n",
      "blocks.25.norm3.bias\n",
      "blocks.25.norm3.weight\n",
      "blocks.25.self_attn.k.bias\n",
      "blocks.25.self_attn.k.weight\n",
      "blocks.25.self_attn.norm_k.weight\n",
      "blocks.25.self_attn.norm_q.weight\n",
      "blocks.25.self_attn.o.bias\n",
      "blocks.25.self_attn.o.weight\n",
      "blocks.25.self_attn.q.bias\n",
      "blocks.25.self_attn.q.weight\n",
      "blocks.25.self_attn.v.bias\n",
      "blocks.25.self_attn.v.weight\n",
      "blocks.26.cross_attn.k.bias\n",
      "blocks.26.cross_attn.k.weight\n",
      "blocks.26.cross_attn.k_img.bias\n",
      "blocks.26.cross_attn.k_img.weight\n",
      "blocks.26.cross_attn.norm_k.weight\n",
      "blocks.26.cross_attn.norm_k_img.weight\n",
      "blocks.26.cross_attn.norm_q.weight\n",
      "blocks.26.cross_attn.o.bias\n",
      "blocks.26.cross_attn.o.weight\n",
      "blocks.26.cross_attn.q.bias\n",
      "blocks.26.cross_attn.q.weight\n",
      "blocks.26.cross_attn.v.bias\n",
      "blocks.26.cross_attn.v.weight\n",
      "blocks.26.cross_attn.v_img.bias\n",
      "blocks.26.cross_attn.v_img.weight\n",
      "blocks.26.ffn.0.bias\n",
      "blocks.26.ffn.0.weight\n",
      "blocks.26.ffn.2.bias\n",
      "blocks.26.ffn.2.weight\n",
      "blocks.26.modulation\n",
      "blocks.26.norm3.bias\n",
      "blocks.26.norm3.weight\n",
      "blocks.26.self_attn.k.bias\n",
      "blocks.26.self_attn.k.weight\n",
      "blocks.26.self_attn.norm_k.weight\n",
      "blocks.26.self_attn.norm_q.weight\n",
      "blocks.26.self_attn.o.bias\n",
      "blocks.26.self_attn.o.weight\n",
      "blocks.26.self_attn.q.bias\n",
      "blocks.26.self_attn.q.weight\n",
      "blocks.26.self_attn.v.bias\n",
      "blocks.26.self_attn.v.weight\n",
      "blocks.27.cross_attn.k.bias\n",
      "blocks.27.cross_attn.k.weight\n",
      "blocks.27.cross_attn.k_img.bias\n",
      "blocks.27.cross_attn.k_img.weight\n",
      "blocks.27.cross_attn.norm_k.weight\n",
      "blocks.27.cross_attn.norm_k_img.weight\n",
      "blocks.27.cross_attn.norm_q.weight\n",
      "blocks.27.cross_attn.o.bias\n",
      "blocks.27.cross_attn.o.weight\n",
      "blocks.27.cross_attn.q.bias\n",
      "blocks.27.cross_attn.q.weight\n",
      "blocks.27.cross_attn.v.bias\n",
      "blocks.27.cross_attn.v.weight\n",
      "blocks.27.cross_attn.v_img.bias\n",
      "blocks.27.cross_attn.v_img.weight\n",
      "blocks.27.ffn.0.bias\n",
      "blocks.27.ffn.0.weight\n",
      "blocks.27.ffn.2.bias\n",
      "blocks.27.ffn.2.weight\n",
      "blocks.27.modulation\n",
      "blocks.27.norm3.bias\n",
      "blocks.27.norm3.weight\n",
      "blocks.27.self_attn.k.bias\n",
      "blocks.27.self_attn.k.weight\n",
      "blocks.27.self_attn.norm_k.weight\n",
      "blocks.27.self_attn.norm_q.weight\n",
      "blocks.27.self_attn.o.bias\n",
      "blocks.27.self_attn.o.weight\n",
      "blocks.27.self_attn.q.bias\n",
      "blocks.27.self_attn.q.weight\n",
      "blocks.27.self_attn.v.bias\n",
      "blocks.27.self_attn.v.weight\n",
      "blocks.28.cross_attn.k.bias\n",
      "blocks.28.cross_attn.k.weight\n",
      "blocks.28.cross_attn.k_img.bias\n",
      "blocks.28.cross_attn.k_img.weight\n",
      "blocks.28.cross_attn.norm_k.weight\n",
      "blocks.28.cross_attn.norm_k_img.weight\n",
      "blocks.28.cross_attn.norm_q.weight\n",
      "blocks.28.cross_attn.o.bias\n",
      "blocks.28.cross_attn.o.weight\n",
      "blocks.28.cross_attn.q.bias\n",
      "blocks.28.cross_attn.q.weight\n",
      "blocks.28.cross_attn.v.bias\n",
      "blocks.28.cross_attn.v.weight\n",
      "blocks.28.cross_attn.v_img.bias\n",
      "blocks.28.cross_attn.v_img.weight\n",
      "blocks.28.ffn.0.bias\n",
      "blocks.28.ffn.0.weight\n",
      "blocks.28.ffn.2.bias\n",
      "blocks.28.ffn.2.weight\n",
      "blocks.28.modulation\n",
      "blocks.28.norm3.bias\n",
      "blocks.28.norm3.weight\n",
      "blocks.28.self_attn.k.bias\n",
      "blocks.28.self_attn.k.weight\n",
      "blocks.28.self_attn.norm_k.weight\n",
      "blocks.28.self_attn.norm_q.weight\n",
      "blocks.28.self_attn.o.bias\n",
      "blocks.28.self_attn.o.weight\n",
      "blocks.28.self_attn.q.bias\n",
      "blocks.28.self_attn.q.weight\n",
      "blocks.28.self_attn.v.bias\n",
      "blocks.28.self_attn.v.weight\n",
      "blocks.29.cross_attn.k.bias\n",
      "blocks.29.cross_attn.k.weight\n",
      "blocks.29.cross_attn.k_img.bias\n",
      "blocks.29.cross_attn.k_img.weight\n",
      "blocks.29.cross_attn.norm_k.weight\n",
      "blocks.29.cross_attn.norm_k_img.weight\n",
      "blocks.29.cross_attn.norm_q.weight\n",
      "blocks.29.cross_attn.o.bias\n",
      "blocks.29.cross_attn.o.weight\n",
      "blocks.29.cross_attn.q.bias\n",
      "blocks.29.cross_attn.q.weight\n",
      "blocks.29.cross_attn.v.bias\n",
      "blocks.29.cross_attn.v.weight\n",
      "blocks.29.cross_attn.v_img.bias\n",
      "blocks.29.cross_attn.v_img.weight\n",
      "blocks.29.ffn.0.bias\n",
      "blocks.29.ffn.0.weight\n",
      "blocks.29.ffn.2.bias\n",
      "blocks.29.ffn.2.weight\n",
      "blocks.29.modulation\n",
      "blocks.29.norm3.bias\n",
      "blocks.29.norm3.weight\n",
      "blocks.29.self_attn.k.bias\n",
      "blocks.29.self_attn.k.weight\n",
      "blocks.29.self_attn.norm_k.weight\n",
      "blocks.29.self_attn.norm_q.weight\n",
      "blocks.29.self_attn.o.bias\n",
      "blocks.29.self_attn.o.weight\n",
      "blocks.29.self_attn.q.bias\n",
      "blocks.29.self_attn.q.weight\n",
      "blocks.29.self_attn.v.bias\n",
      "blocks.29.self_attn.v.weight\n",
      "blocks.3.cross_attn.k.bias\n",
      "blocks.3.cross_attn.k.weight\n",
      "blocks.3.cross_attn.k_img.bias\n",
      "blocks.3.cross_attn.k_img.weight\n",
      "blocks.3.cross_attn.norm_k.weight\n",
      "blocks.3.cross_attn.norm_k_img.weight\n",
      "blocks.3.cross_attn.norm_q.weight\n",
      "blocks.3.cross_attn.o.bias\n",
      "blocks.3.cross_attn.o.weight\n",
      "blocks.3.cross_attn.q.bias\n",
      "blocks.3.cross_attn.q.weight\n",
      "blocks.3.cross_attn.v.bias\n",
      "blocks.3.cross_attn.v.weight\n",
      "blocks.3.cross_attn.v_img.bias\n",
      "blocks.3.cross_attn.v_img.weight\n",
      "blocks.3.ffn.0.bias\n",
      "blocks.3.ffn.0.weight\n",
      "blocks.3.ffn.2.bias\n",
      "blocks.3.ffn.2.weight\n",
      "blocks.3.modulation\n",
      "blocks.3.norm3.bias\n",
      "blocks.3.norm3.weight\n",
      "blocks.3.self_attn.k.bias\n",
      "blocks.3.self_attn.k.weight\n",
      "blocks.3.self_attn.norm_k.weight\n",
      "blocks.3.self_attn.norm_q.weight\n",
      "blocks.3.self_attn.o.bias\n",
      "blocks.3.self_attn.o.weight\n",
      "blocks.3.self_attn.q.bias\n",
      "blocks.3.self_attn.q.weight\n",
      "blocks.3.self_attn.v.bias\n",
      "blocks.3.self_attn.v.weight\n",
      "blocks.4.cross_attn.k.bias\n",
      "blocks.4.cross_attn.k.weight\n",
      "blocks.4.cross_attn.k_img.bias\n",
      "blocks.4.cross_attn.k_img.weight\n",
      "blocks.4.cross_attn.norm_k.weight\n",
      "blocks.4.cross_attn.norm_k_img.weight\n",
      "blocks.4.cross_attn.norm_q.weight\n",
      "blocks.4.cross_attn.o.bias\n",
      "blocks.4.cross_attn.o.weight\n",
      "blocks.4.cross_attn.q.bias\n",
      "blocks.4.cross_attn.q.weight\n",
      "blocks.4.cross_attn.v.bias\n",
      "blocks.4.cross_attn.v.weight\n",
      "blocks.4.cross_attn.v_img.bias\n",
      "blocks.4.cross_attn.v_img.weight\n",
      "blocks.4.ffn.0.bias\n",
      "blocks.4.ffn.0.weight\n",
      "blocks.4.ffn.2.bias\n",
      "blocks.4.ffn.2.weight\n",
      "blocks.4.modulation\n",
      "blocks.4.norm3.bias\n",
      "blocks.4.norm3.weight\n",
      "blocks.4.self_attn.k.bias\n",
      "blocks.4.self_attn.k.weight\n",
      "blocks.4.self_attn.norm_k.weight\n",
      "blocks.4.self_attn.norm_q.weight\n",
      "blocks.4.self_attn.o.bias\n",
      "blocks.4.self_attn.o.weight\n",
      "blocks.4.self_attn.q.bias\n",
      "blocks.4.self_attn.q.weight\n",
      "blocks.4.self_attn.v.bias\n",
      "blocks.4.self_attn.v.weight\n",
      "blocks.5.cross_attn.k.bias\n",
      "blocks.5.cross_attn.k.weight\n",
      "blocks.5.cross_attn.k_img.bias\n",
      "blocks.5.cross_attn.k_img.weight\n",
      "blocks.5.cross_attn.norm_k.weight\n",
      "blocks.5.cross_attn.norm_k_img.weight\n",
      "blocks.5.cross_attn.norm_q.weight\n",
      "blocks.5.cross_attn.o.bias\n",
      "blocks.5.cross_attn.o.weight\n",
      "blocks.5.cross_attn.q.bias\n",
      "blocks.5.cross_attn.q.weight\n",
      "blocks.5.cross_attn.v.bias\n",
      "blocks.5.cross_attn.v.weight\n",
      "blocks.5.cross_attn.v_img.bias\n",
      "blocks.5.cross_attn.v_img.weight\n",
      "blocks.5.ffn.0.bias\n",
      "blocks.5.ffn.0.weight\n",
      "blocks.5.ffn.2.bias\n",
      "blocks.5.ffn.2.weight\n",
      "blocks.5.modulation\n",
      "blocks.5.norm3.bias\n",
      "blocks.5.norm3.weight\n",
      "blocks.5.self_attn.k.bias\n",
      "blocks.5.self_attn.k.weight\n",
      "blocks.5.self_attn.norm_k.weight\n",
      "blocks.5.self_attn.norm_q.weight\n",
      "blocks.5.self_attn.o.bias\n",
      "blocks.5.self_attn.o.weight\n",
      "blocks.5.self_attn.q.bias\n",
      "blocks.5.self_attn.q.weight\n",
      "blocks.5.self_attn.v.bias\n",
      "blocks.5.self_attn.v.weight\n",
      "blocks.6.cross_attn.k.bias\n",
      "blocks.6.cross_attn.k.weight\n",
      "blocks.6.cross_attn.k_img.bias\n",
      "blocks.6.cross_attn.k_img.weight\n",
      "blocks.6.cross_attn.norm_k.weight\n",
      "blocks.6.cross_attn.norm_k_img.weight\n",
      "blocks.6.cross_attn.norm_q.weight\n",
      "blocks.6.cross_attn.o.bias\n",
      "blocks.6.cross_attn.o.weight\n",
      "blocks.6.cross_attn.q.bias\n",
      "blocks.6.cross_attn.q.weight\n",
      "blocks.6.cross_attn.v.bias\n",
      "blocks.6.cross_attn.v.weight\n",
      "blocks.6.cross_attn.v_img.bias\n",
      "blocks.6.cross_attn.v_img.weight\n",
      "blocks.6.ffn.0.bias\n",
      "blocks.6.ffn.0.weight\n",
      "blocks.6.ffn.2.bias\n",
      "blocks.6.ffn.2.weight\n",
      "blocks.6.modulation\n",
      "blocks.6.norm3.bias\n",
      "blocks.6.norm3.weight\n",
      "blocks.6.self_attn.k.bias\n",
      "blocks.6.self_attn.k.weight\n",
      "blocks.6.self_attn.norm_k.weight\n",
      "blocks.6.self_attn.norm_q.weight\n",
      "blocks.6.self_attn.o.bias\n",
      "blocks.6.self_attn.o.weight\n",
      "blocks.6.self_attn.q.bias\n",
      "blocks.6.self_attn.q.weight\n",
      "blocks.6.self_attn.v.bias\n",
      "blocks.6.self_attn.v.weight\n",
      "blocks.7.cross_attn.k.bias\n",
      "blocks.7.cross_attn.k.weight\n",
      "blocks.7.cross_attn.k_img.bias\n",
      "blocks.7.cross_attn.k_img.weight\n",
      "blocks.7.cross_attn.norm_k.weight\n",
      "blocks.7.cross_attn.norm_k_img.weight\n",
      "blocks.7.cross_attn.norm_q.weight\n",
      "blocks.7.cross_attn.o.bias\n",
      "blocks.7.cross_attn.o.weight\n",
      "blocks.7.cross_attn.q.bias\n",
      "blocks.7.cross_attn.q.weight\n",
      "blocks.7.cross_attn.v.bias\n",
      "blocks.7.cross_attn.v.weight\n",
      "blocks.7.cross_attn.v_img.bias\n",
      "blocks.7.cross_attn.v_img.weight\n",
      "blocks.7.ffn.0.bias\n",
      "blocks.7.ffn.0.weight\n",
      "blocks.7.ffn.2.bias\n",
      "blocks.7.ffn.2.weight\n",
      "blocks.7.modulation\n",
      "blocks.7.norm3.bias\n",
      "blocks.7.norm3.weight\n",
      "blocks.7.self_attn.k.bias\n",
      "blocks.7.self_attn.k.weight\n",
      "blocks.7.self_attn.norm_k.weight\n",
      "blocks.7.self_attn.norm_q.weight\n",
      "blocks.7.self_attn.o.bias\n",
      "blocks.7.self_attn.o.weight\n",
      "blocks.7.self_attn.q.bias\n",
      "blocks.7.self_attn.q.weight\n",
      "blocks.7.self_attn.v.bias\n",
      "blocks.7.self_attn.v.weight\n",
      "blocks.8.cross_attn.k.bias\n",
      "blocks.8.cross_attn.k.weight\n",
      "blocks.8.cross_attn.k_img.bias\n",
      "blocks.8.cross_attn.k_img.weight\n",
      "blocks.8.cross_attn.norm_k.weight\n",
      "blocks.8.cross_attn.norm_k_img.weight\n",
      "blocks.8.cross_attn.norm_q.weight\n",
      "blocks.8.cross_attn.o.bias\n",
      "blocks.8.cross_attn.o.weight\n",
      "blocks.8.cross_attn.q.bias\n",
      "blocks.8.cross_attn.q.weight\n",
      "blocks.8.cross_attn.v.bias\n",
      "blocks.8.cross_attn.v.weight\n",
      "blocks.8.cross_attn.v_img.bias\n",
      "blocks.8.cross_attn.v_img.weight\n",
      "blocks.8.ffn.0.bias\n",
      "blocks.8.ffn.0.weight\n",
      "blocks.8.ffn.2.bias\n",
      "blocks.8.ffn.2.weight\n",
      "blocks.8.modulation\n",
      "blocks.8.norm3.bias\n",
      "blocks.8.norm3.weight\n",
      "blocks.8.self_attn.k.bias\n",
      "blocks.8.self_attn.k.weight\n",
      "blocks.8.self_attn.norm_k.weight\n",
      "blocks.8.self_attn.norm_q.weight\n",
      "blocks.8.self_attn.o.bias\n",
      "blocks.8.self_attn.o.weight\n",
      "blocks.8.self_attn.q.bias\n",
      "blocks.8.self_attn.q.weight\n",
      "blocks.8.self_attn.v.bias\n",
      "blocks.8.self_attn.v.weight\n",
      "blocks.9.cross_attn.k.bias\n",
      "blocks.9.cross_attn.k.weight\n",
      "blocks.9.cross_attn.k_img.bias\n",
      "blocks.9.cross_attn.k_img.weight\n",
      "blocks.9.cross_attn.norm_k.weight\n",
      "blocks.9.cross_attn.norm_k_img.weight\n",
      "blocks.9.cross_attn.norm_q.weight\n",
      "blocks.9.cross_attn.o.bias\n",
      "blocks.9.cross_attn.o.weight\n",
      "blocks.9.cross_attn.q.bias\n",
      "blocks.9.cross_attn.q.weight\n",
      "blocks.9.cross_attn.v.bias\n",
      "blocks.9.cross_attn.v.weight\n",
      "blocks.9.cross_attn.v_img.bias\n",
      "blocks.9.cross_attn.v_img.weight\n",
      "blocks.9.ffn.0.bias\n",
      "blocks.9.ffn.0.weight\n",
      "blocks.9.ffn.2.bias\n",
      "blocks.9.ffn.2.weight\n",
      "blocks.9.modulation\n",
      "blocks.9.norm3.bias\n",
      "blocks.9.norm3.weight\n",
      "blocks.9.self_attn.k.bias\n",
      "blocks.9.self_attn.k.weight\n",
      "blocks.9.self_attn.norm_k.weight\n",
      "blocks.9.self_attn.norm_q.weight\n",
      "blocks.9.self_attn.o.bias\n",
      "blocks.9.self_attn.o.weight\n",
      "blocks.9.self_attn.q.bias\n",
      "blocks.9.self_attn.q.weight\n",
      "blocks.9.self_attn.v.bias\n",
      "blocks.9.self_attn.v.weight\n",
      "head.head.bias\n",
      "head.head.weight\n",
      "head.modulation\n",
      "img_emb.proj.0.bias\n",
      "img_emb.proj.0.weight\n",
      "img_emb.proj.1.bias\n",
      "img_emb.proj.1.weight\n",
      "img_emb.proj.3.bias\n",
      "img_emb.proj.3.weight\n",
      "img_emb.proj.4.bias\n",
      "img_emb.proj.4.weight\n",
      "patch_embedding.bias\n",
      "patch_embedding.weight\n",
      "text_embedding.0.bias\n",
      "text_embedding.0.weight\n",
      "text_embedding.2.bias\n",
      "text_embedding.2.weight\n",
      "time_embedding.0.bias\n",
      "time_embedding.0.weight\n",
      "time_embedding.2.bias\n",
      "time_embedding.2.weight\n",
      "time_projection.1.bias\n",
      "time_projection.1.weight\n"
     ]
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"/home/kaiwenguo/dev/models/Wan2.1-Fun-1.3B-InP/diffusion_pytorch_model.safetensors\"\n",
    ").resolve()\n",
    "state_dict = load_file(checkpoint_path)\n",
    "for k, _ in state_dict.items():\n",
    "    print(k)\n",
    "# new_state_dict = from_wan_dit_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/tmp_kaiwenguo/_s3/synthesia-rnd-prd-third-party-models/wan/dit-ref-modulation/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 2, kind: NotFound, message: \"No such file or directory\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m new_checkpoint_path = OutputPath(\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33ms3://synthesia-rnd-prd-third-party-models/wan/dit-ref-modulation/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(new_checkpoint_path.resolve())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_checkpoint_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m new_checkpoint_path.commit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/nb/.venv/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 2, kind: NotFound, message: \"No such file or directory\" })"
     ]
    }
   ],
   "source": [
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit-ref-modulation/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\"\n",
    ")\n",
    "print(new_checkpoint_path.resolve())\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Wan2.1-Fun-1.3B-InP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CloudOutputPath('s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-bfloat16-fix.safetensors')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"/home/kaiwenguo/dev/models/Wan2.1-Fun-1.3B-InP/diffusion_pytorch_model.safetensors\"\n",
    ").resolve()\n",
    "state_dict = load_file(checkpoint_path)\n",
    "new_state_dict = from_wan_dit_state_dict(state_dict)\n",
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-bfloat16-fix.safetensors\"\n",
    ")\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Wan2.1-I2V-14B-480P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|| 9/9 [00:02<00:00,  3.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CloudOutputPath('s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-14B-480P-bfloat16-noref-fix.safetensors')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/Wan2.1-I2V-14B-480P/\"\n",
    ").resolve()\n",
    "state_dicts = {}\n",
    "for checkpoint_path in checkpoint_path.glob(\"*.safetensors\"):\n",
    "    state_dicts.update(load_file(checkpoint_path))\n",
    "new_state_dict = from_wan_dit_state_dict(state_dicts)\n",
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-14B-480P-bfloat16-noref-fix.safetensors\"\n",
    ")\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|| 9/9 [03:55<00:00, 26.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CloudOutputPath('s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-14B-720P-bfloat16-noref-fix.safetensors')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/Wan2.1-I2V-14B-720P/\"\n",
    ").resolve()\n",
    "state_dicts = {}\n",
    "for checkpoint_path in checkpoint_path.glob(\"*.safetensors\"):\n",
    "    state_dicts.update(load_file(checkpoint_path))\n",
    "new_state_dict = from_wan_dit_state_dict(state_dicts)\n",
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-14B-720P-bfloat16-noref-fix.safetensors\"\n",
    ")\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Wan2.1-T2V-1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|| 2/2 [00:22<00:00, 11.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CloudOutputPath('s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-T2V-1.3B-480P-bfloat16-fix.safetensors')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/Wan2.1-T2V-1.3B/\"\n",
    ").resolve()\n",
    "state_dicts = {}\n",
    "for checkpoint_path in checkpoint_path.glob(\"*.safetensors\"):\n",
    "    state_dicts.update(load_file(checkpoint_path))\n",
    "new_state_dict = from_wan_dit_state_dict(state_dicts)\n",
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-T2V-1.3B-480P-bfloat16-fix.safetensors\"\n",
    ")\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Wan2.1-Fun-V1.1-1.3B-InP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CloudOutputPath('s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-V1.1-bfloat16.safetensors')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "from ditwo.utils.pretrained import from_wan_dit_state_dict\n",
    "\n",
    "checkpoint_path = InputPath(\n",
    "    \"/home/kaiwenguo/dev/models/Wan2.1-Fun-V1.1-1.3B-InP/diffusion_pytorch_model.safetensors\"\n",
    ").resolve()\n",
    "state_dict = load_file(checkpoint_path)\n",
    "new_state_dict = from_wan_dit_state_dict(state_dict)\n",
    "new_checkpoint_path = OutputPath(\n",
    "    \"s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-V1.1-bfloat16.safetensors\"\n",
    ")\n",
    "save_file(new_state_dict, new_checkpoint_path.resolve())\n",
    "new_checkpoint_path.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
