{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/kaiwenguo/dev/nb\n",
      "Hostname: p5-48xlarge-dy-p5-48xlarge-3\n",
      "Python Executable: /home/kaiwenguo/dev/nb/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "print(f\"Hostname: {socket.gethostname()}\")\n",
    "print(f\"Python Executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Device count: 8\n",
      "Current device: 0\n",
      "Device name: NVIDIA H100 80GB HBM3\n",
      "CUDA_VISIBLE_DEVICES: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/kaiwenguo/dev/rnd-ditwo/src\")\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "# You can also check the value of the environment variable directly from within the kernel\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:01:40,819] [INFO] [train.py:149] Running experiment in Cloud environment `CloudEnvironment.AWS`\n",
      "[2025-04-25 22:01:41,059] [INFO] [scaling.py:474] Rank 0, data parallel group: [0]\n",
      "[2025-04-25 22:01:41,060] [INFO] [scaling.py:475] Rank 0, sequence parallel group: [0]\n",
      "[2025-04-25 22:01:41,080] [INFO] [train.py:294] No checkpoint specified and no 'latest' checkpoint found. Starting training from scratch.\n",
      "[2025-04-25 22:01:41,081] [INFO] [train.py:185] Graceful shutdown signal handler (SIGUSR1) registered.\n",
      "[2025-04-25 22:01:41,081] [INFO] [train.py:190] ****** Setting up random seeds ******\n",
      "[2025-04-25 22:01:41,082] [INFO] [train.py:191]     Base random seed: 42\n",
      "[2025-04-25 22:01:41,088] [INFO] [train.py:196]     Random seed set to 42 on device cuda:0\n",
      "[2025-04-25 22:01:41,088] [INFO] [train.py:204] ****** Creating models ******\n",
      "[2025-04-25 22:01:41,089] [INFO] [train.py:205]   Pretrained DiT Path: s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\n",
      "[2025-04-25 22:01:41,089] [INFO] [train.py:206]   Pretrained VAE Path: s3://synthesia-rnd-prd-third-party-models/wan/vae/wan_vae_converted_dit.safetensors\n",
      "[2025-04-25 22:01:41,089] [INFO] [wan.py:248] ðŸš€ Attention used: PytorchSDPA ðŸš€\n",
      "[2025-04-25 22:01:41,366] [INFO] [train.py:218] Loading WAN weights from: s3://synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\n",
      "[2025-04-25 22:01:41,712] [INFO] [train.py:424] Loading WAN weights from resolved path: /scratch/tmp_kaiwenguo/_s3/synthesia-rnd-prd-third-party-models/wan/dit/Wan2.1-I2V-1.3B-multires-bfloat16.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:01:55,099] [INFO] [train.py:249] ****** AdamW Optimizer ******\n",
      "[2025-04-25 22:01:55,100] [INFO] [train.py:250]   Learning Rate: 2e-05\n",
      "[2025-04-25 22:01:55,101] [INFO] [train.py:251]   Betas: (0.9, 0.95)\n",
      "[2025-04-25 22:01:55,101] [INFO] [train.py:252]   Weight Decay: 0.001\n",
      "[2025-04-25 22:01:55,104] [INFO] [train.py:264] ******** LR Scheduler ********\n",
      "[2025-04-25 22:01:55,104] [INFO] [train.py:265]   Type: constant_with_deep_warmup\n",
      "[2025-04-25 22:01:55,105] [INFO] [train.py:267]   Warmup Steps: 180\n",
      "[2025-04-25 22:01:55,106] [INFO] [train.py:315] ****** Preparing for Distributed Training ******\n",
      "[2025-04-25 22:01:55,106] [INFO] [train.py:316]   Using Torch Compile: False\n",
      "[2025-04-25 22:01:55,106] [INFO] [train.py:317]   Mixed Precision: bf16\n",
      "[2025-04-25 22:01:55,107] [INFO] [train.py:318]   Activation Checkpointing: True\n",
      "[2025-04-25 22:01:55,107] [INFO] [train.py:319]   Batch Size per GPU: 1\n",
      "[2025-04-25 22:01:55,107] [INFO] [train.py:320]   Total Batch Size: 256\n",
      "[2025-04-25 22:01:55,108] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-25 22:01:55,160] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-25 22:01:55,162] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-04-25 22:01:55,162] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-25 22:01:55,230] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-04-25 22:01:55,231] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2025-04-25 22:01:55,231] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-04-25 22:01:55,231] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 400000\n",
      "[2025-04-25 22:01:55,232] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 400000\n",
      "[2025-04-25 22:01:55,232] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False\n",
      "[2025-04-25 22:01:55,233] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/10000000 [10:06<?, ?samples/s , loss=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:01:58,955] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-25 22:01:58,957] [INFO] [utils.py:782:see_memory_usage] MA 37.59 GB         Max_MA 40.41 GB         CA 41.18 GB         Max_CA 41 GB \n",
      "[2025-04-25 22:01:58,958] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 213.08 GB, percent = 10.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:01:59,571] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-25 22:01:59,573] [INFO] [utils.py:782:see_memory_usage] MA 37.59 GB         Max_MA 42.87 GB         CA 46.47 GB         Max_CA 46 GB \n",
      "[2025-04-25 22:01:59,574] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 212.82 GB, percent = 10.6%\n",
      "[2025-04-25 22:01:59,574] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized\n",
      "[2025-04-25 22:02:00,189] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-25 22:02:00,190] [INFO] [utils.py:782:see_memory_usage] MA 37.59 GB         Max_MA 37.59 GB         CA 46.47 GB         Max_CA 46 GB \n",
      "[2025-04-25 22:02:00,192] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 214.22 GB, percent = 10.7%\n",
      "[2025-04-25 22:02:00,208] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-04-25 22:02:00,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-04-25 22:02:00,209] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7ee9fd303830>\n",
      "[2025-04-25 22:02:00,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]\n",
      "[2025-04-25 22:02:00,472] [INFO] [config.py:997:print] DeepSpeedEngine configuration:\n",
      "[2025-04-25 22:02:00,474] [INFO] [config.py:1001:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-04-25 22:02:00,479] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2025-04-25 22:02:00,480] [INFO] [config.py:1001:print]   amp_enabled .................. False\n",
      "[2025-04-25 22:02:00,481] [INFO] [config.py:1001:print]   amp_params ................... False\n",
      "[2025-04-25 22:02:00,484] [INFO] [config.py:1001:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-04-25 22:02:00,485] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True\n",
      "[2025-04-25 22:02:00,485] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-04-25 22:02:00,486] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-04-25 22:02:00,487] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-04-25 22:02:00,488] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-04-25 22:02:00,489] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ee9fd2459d0>\n",
      "[2025-04-25 22:02:00,490] [INFO] [config.py:1001:print]   communication_data_type ...... None\n",
      "[2025-04-25 22:02:00,491] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-04-25 22:02:00,492] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False\n",
      "[2025-04-25 22:02:00,493] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False\n",
      "[2025-04-25 22:02:00,493] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-04-25 22:02:00,494] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False\n",
      "[2025-04-25 22:02:00,495] [INFO] [config.py:1001:print]   dataloader_drop_last ......... True\n",
      "[2025-04-25 22:02:00,496] [INFO] [config.py:1001:print]   disable_allgather ............ False\n",
      "[2025-04-25 22:02:00,496] [INFO] [config.py:1001:print]   dump_state ................... False\n",
      "[2025-04-25 22:02:00,496] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-04-25 22:02:00,497] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False\n",
      "[2025-04-25 22:02:00,497] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-04-25 22:02:00,497] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-04-25 22:02:00,498] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-04-25 22:02:00,498] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-04-25 22:02:00,499] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-04-25 22:02:00,499] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-04-25 22:02:00,500] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False\n",
      "[2025-04-25 22:02:00,500] [INFO] [config.py:1001:print]   elasticity_enabled ........... False\n",
      "[2025-04-25 22:02:00,500] [INFO] [config.py:1001:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-04-25 22:02:00,501] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None\n",
      "[2025-04-25 22:02:00,501] [INFO] [config.py:1001:print]   fp16_enabled ................. False\n",
      "[2025-04-25 22:02:00,502] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-04-25 22:02:00,502] [INFO] [config.py:1001:print]   global_rank .................. 0\n",
      "[2025-04-25 22:02:00,503] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None\n",
      "[2025-04-25 22:02:00,503] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 256\n",
      "[2025-04-25 22:02:00,503] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0\n",
      "[2025-04-25 22:02:00,504] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-04-25 22:02:00,504] [INFO] [config.py:1001:print]   graph_harvesting ............. False\n",
      "[2025-04-25 22:02:00,504] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-04-25 22:02:00,505] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1\n",
      "[2025-04-25 22:02:00,505] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False\n",
      "[2025-04-25 22:02:00,505] [INFO] [config.py:1001:print]   loss_scale ................... 1.0\n",
      "[2025-04-25 22:02:00,506] [INFO] [config.py:1001:print]   memory_breakdown ............. False\n",
      "[2025-04-25 22:02:00,506] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False\n",
      "[2025-04-25 22:02:00,506] [INFO] [config.py:1001:print]   mics_shard_size .............. -1\n",
      "[2025-04-25 22:02:00,507] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2025-04-25 22:02:00,507] [INFO] [config.py:1001:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-04-25 22:02:00,508] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-04-25 22:02:00,508] [INFO] [config.py:1001:print]   optimizer_name ............... None\n",
      "[2025-04-25 22:02:00,510] [INFO] [config.py:1001:print]   optimizer_params ............. None\n",
      "[2025-04-25 22:02:00,510] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-04-25 22:02:00,510] [INFO] [config.py:1001:print]   pld_enabled .................. False\n",
      "[2025-04-25 22:02:00,511] [INFO] [config.py:1001:print]   pld_params ................... False\n",
      "[2025-04-25 22:02:00,511] [INFO] [config.py:1001:print]   prescale_gradients ........... False\n",
      "[2025-04-25 22:02:00,512] [INFO] [config.py:1001:print]   scheduler_name ............... None\n",
      "[2025-04-25 22:02:00,512] [INFO] [config.py:1001:print]   scheduler_params ............. None\n",
      "[2025-04-25 22:02:00,512] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-04-25 22:02:00,513] [INFO] [config.py:1001:print]   sparse_attention ............. None\n",
      "[2025-04-25 22:02:00,513] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False\n",
      "[2025-04-25 22:02:00,513] [INFO] [config.py:1001:print]   steps_per_print .............. 10000000000.0\n",
      "[2025-04-25 22:02:00,514] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-04-25 22:02:00,514] [INFO] [config.py:1001:print]   train_batch_size ............. 256\n",
      "[2025-04-25 22:02:00,514] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-04-25 22:02:00,515] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False\n",
      "[2025-04-25 22:02:00,515] [INFO] [config.py:1001:print]   use_node_local_storage ....... False\n",
      "[2025-04-25 22:02:00,515] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False\n",
      "[2025-04-25 22:02:00,516] [INFO] [config.py:1001:print]   weight_quantization_config ... None\n",
      "[2025-04-25 22:02:00,517] [INFO] [config.py:1001:print]   world_size ................... 1\n",
      "[2025-04-25 22:02:00,517] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True\n",
      "[2025-04-25 22:02:00,518] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=True reduce_bucket_size=400000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=400000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-04-25 22:02:00,518] [INFO] [config.py:1001:print]   zero_enabled ................. True\n",
      "[2025-04-25 22:02:00,518] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-04-25 22:02:00,519] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2\n",
      "[2025-04-25 22:02:00,519] [INFO] [config.py:987:print_user_config]   json = {\n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"dataloader_drop_last\": true, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"reduce_bucket_size\": 4.000000e+05, \n",
      "        \"allgather_bucket_size\": 4.000000e+05, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": false\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"cpu_checkpointing\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"steps_per_print\": 1.000000e+10, \n",
      "    \"checkpoint\": {\n",
      "        \"use_node_local_storage\": false\n",
      "    }, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"train_batch_size\": 256, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[2025-04-25 22:02:00,520] [INFO] [checkpointing.py:1049:_configure_using_config_file] {'partition_activations': False, 'contiguous_memory_optimization': False, 'cpu_checkpointing': False, 'number_checkpoints': None, 'synchronize_checkpoint_boundary': False, 'profile': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:02:01,324] [INFO] [train.py:346] ***** Creating Training Dataset *****\n",
      "[2025-04-25 22:02:01,324] [INFO] [train.py:347]   Index file (original): s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:lvm_dataclasses.data.frames_datasets.video_dataset:Filtered out 45436 clips that are not compatible due to insufficient number of frames and/or incompatible frame rates. Consider using a different snapshot or changing the target frame settings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:02:01,407] [INFO] [train.py:391] Training dataset and dataloader created in 0.88s\n",
      "[2025-04-25 22:02:01,408] [INFO] [train.py:392]   Dataset size: 27082 samples\n",
      "[2025-04-25 22:02:01,408] [INFO] [train.py:393]   Frames per sample: 81\n",
      "[2025-04-25 22:02:01,409] [INFO] [train.py:394]   Number of workers: 4\n",
      "[2025-04-25 22:02:01,409] [INFO] [train.py:606] Setting up Inference Pipeline for validation...\n",
      "[2025-04-25 22:02:01,428] [INFO] [train.py:622] Inference Pipeline setup complete. Memory usage change: 0.00 MB\n",
      "[2025-04-25 22:02:01,429] [INFO] [train.py:623] Inference Pipeline setup complete.\n",
      "[2025-04-25 22:02:01,429] [INFO] [train.py:399] ****** Setting up Validation Modules ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [on]\n",
      "Loading model from: /home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "[2025-04-25 22:02:02,686] [INFO] [train.py:411] ****** Setting up Experiment Tracker ******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/25 22:02:02 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/04/25 22:02:02 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/04/25 22:02:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/04/25 22:02:03 WARNING mlflow.utils.autologging_utils: MLflow transformers autologging is known to be compatible with 4.25.1 <= transformers <= 4.48.0, but the installed version is 4.51.3. If you encounter errors during autologging, try upgrading / downgrading transformers to a compatible version, or try upgrading MLflow.\n",
      "2025/04/25 22:02:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
      "ERROR:experiment_tracking.logging:Failed to start run for SupportedBackend.MLFLOW: Run with UUID 93e274cf7e4840bf9bffc0ecef6277ee is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True. The rest of the backends will still be enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:02:03,384] [INFO] [train.py:414] Experiment tracker initialized.\n"
     ]
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from ditwo.train import Trainer\n",
    "from ditwo.configs.config_dataclasses import DiTTrainingConfig, WAN_I2V_1_3B_CONFIG\n",
    "from ditwo.constants import WAN_FUN_I2V_1_3B_MULTIRES_MODEL_PATH\n",
    "from ditwo.utils.config import update_config\n",
    "\n",
    "\n",
    "index_file = InputPath(\n",
    "    \"s3://synthesia-rnd-dataops-prd-datalake/dataset-snapshots/data/snapshot_f8fb1298-a5fe-4413-b97c-cc38503bf0d5/\"\n",
    ")\n",
    "output_dir = OutputPath(\"s3://synthesia-rnd-eun1-experiments/ditwo/kaiwen/pose_guidance_001\")\n",
    "\n",
    "\n",
    "config = DiTTrainingConfig()\n",
    "config_override = {\n",
    "    \"dit_model_config\": WAN_I2V_1_3B_CONFIG.model_dump(),\n",
    "    \"pretrained_dit_state_dict_path\": WAN_FUN_I2V_1_3B_MULTIRES_MODEL_PATH,\n",
    "}\n",
    "config = update_config(config, config_override)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(\n",
    "    index_file=index_file,\n",
    "    experiment_dir=output_dir,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 19.018150329589844 GB\n",
      "Memory cached: 19.49609375 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**3} GB\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_reserved() / 1024**3} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:08:21,027] [INFO] [train.py:597] No checkpoint found to resume from. Starting training from step 0.\n",
      "[2025-04-25 22:08:21,028] [INFO] [train.py:936] ***** Starting Validation Run at Step 0 *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.10it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.02it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.88it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.12it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.87it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.64it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.09it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.30it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.95it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.95it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.28it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.25it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.06it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.13it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.87it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.80it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:08:52,535] [ERROR] [train.py:1033] Error processing validation clip c38d56ba-f404-31ad-832b-3e305b0485fc: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:08:54,836] [ERROR] [train.py:1033] Error processing validation clip f0b3fdd3-b141-336e-972d-50db231c53b8: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:08:56,963] [ERROR] [train.py:1033] Error processing validation clip 2e0d51d7-faff-3dc5-84e6-8627ad669b11: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:08:58,921] [ERROR] [train.py:1033] Error processing validation clip b3505c78-78d6-38f4-a136-95385a41e9f8: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:00,904] [ERROR] [train.py:1033] Error processing validation clip 4fdb9bb7-439c-3b50-a2a5-d8cc46583882: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:03,643] [ERROR] [train.py:1033] Error processing validation clip 8930cc03-8c27-3f5b-b57d-0adb14962093: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:05,881] [ERROR] [train.py:1033] Error processing validation clip 5d6caf20-9d2f-3eb0-870a-51f51611a467: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:08,191] [ERROR] [train.py:1033] Error processing validation clip c3c60d66-b7ff-3b98-be1f-bba90c957b48: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:10,393] [ERROR] [train.py:1033] Error processing validation clip 9266d7e6-ac8a-3bfc-ac65-0c36df7fa1ca: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:12,340] [ERROR] [train.py:1033] Error processing validation clip fd05a8ab-8474-3018-ab97-0a1e5e11155d: 'DiTInference' object has no attribute 'control_dataset_config'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/train.py\", line 1006, in log_validation\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/eval/benchmark.py\", line 262, in get_benchmark_assets\n",
      "    outputs = inference_pipeline.predict_latents(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/nb/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaiwenguo/dev/rnd-ditwo/src/ditwo/utils/inference_pipeline.py\", line 274, in predict_latents\n",
      "    print(f\"inputs.control_frames: {inputs.control_frames}\")\n",
      "                                                             \n",
      "AttributeError: 'DiTInference' object has no attribute 'control_dataset_config'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Denoising validation videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  0.45samples/s ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:12,469] [INFO] [train.py:1054] Aggregating metrics from 10 validation clips across all nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:12,767] [INFO] [train.py:1061] Average Validation Metrics: n_pixels=0 mse=0.0 psnr=0.0 lpips=0.0 var_lap=0.0\n",
      "[2025-04-25 22:09:12,768] [INFO] [train.py:1085] Validation run finished in 51.74s\n",
      "[2025-04-25 22:09:12,770] [INFO] [train.py:628] ****** Setting up Profiler (Enabled: False) ******\n",
      "[2025-04-25 22:09:12,771] [INFO] [train.py:643] ***** Running training *****\n",
      "[2025-04-25 22:09:12,772] [INFO] [train.py:644]   Number distinct videos = 27082\n",
      "[2025-04-25 22:09:12,772] [INFO] [train.py:645]   Number batches per epoch = 27082\n",
      "[2025-04-25 22:09:12,772] [INFO] [train.py:647]   Target epochs =  369.25 (based on distinct videos)\n",
      "[2025-04-25 22:09:12,773] [INFO] [train.py:648]   Note: An epoch traverses one sequence sample per video.\n",
      "[2025-04-25 22:09:12,773] [INFO] [train.py:649]   Total batch size (w. parallel, distributed & accumulation) = 256\n",
      "[2025-04-25 22:09:12,773] [INFO] [train.py:652]   Batch size per GPU = 1\n",
      "[2025-04-25 22:09:12,773] [INFO] [train.py:653]   Gradient Accumulation steps = 256\n",
      "[2025-04-25 22:09:12,774] [INFO] [train.py:654]   Sequence Parallel size = 1\n",
      "[2025-04-25 22:09:12,774] [INFO] [train.py:655]   Target optimization steps = 39063\n",
      "[2025-04-25 22:09:12,776] [INFO] [train.py:658]   Total trainable parameters = 1,418,933,824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-25 22:09:12,781] [INFO] [train.py:473] ************* Starting Training Loop *************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.forward_and_compute_loss() missing 2 required keyword-only arguments: 'reference_frames' and 'control_frames'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mditwo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WAN_FUN_I2V_1_3B_MULTIRES_MODEL_PATH\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mditwo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m update_config\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rnd-ditwo/src/ditwo/train.py:487\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Exit loop after sync, state will be saved below\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# Perform one training step (forward, loss, backward, optimizer step)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m current_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;66;03m# --- Post-Step Operations ---\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# Only perform these actions when an optimization step actually happens\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_wrapper.is_gradient_accumulation_boundary():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rnd-ditwo/src/ditwo/train.py:684\u001b[39m, in \u001b[36mTrainer._training_step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    681\u001b[39m     frame_offsets = torch.tensor([chunk_offset] * frames.size(\u001b[32m0\u001b[39m), device=\u001b[38;5;28mself\u001b[39m.runtime.device)\n\u001b[32m    683\u001b[39m \u001b[38;5;66;03m# --- Forward Pass and Loss Computation ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_and_compute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pass other required inputs like reference_frames, control_frames if needed by the forward pass\u001b[39;49;00m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframe_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_shift_value\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime_shift_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfirst_frame_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst_frame_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43msp_group\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43msp_group\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_sequence_parallel_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# --- Backward Pass and Optimizer Step ---\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# DeepSpeed handles gradient accumulation and scaling internally via `backward` and `step`\u001b[39;00m\n\u001b[32m    695\u001b[39m \u001b[38;5;28mself\u001b[39m.model_wrapper.backward(losses.velocity_loss)\n",
      "\u001b[31mTypeError\u001b[39m: Trainer.forward_and_compute_loss() missing 2 required keyword-only arguments: 'reference_frames' and 'control_frames'"
     ]
    }
   ],
   "source": [
    "from datalib import InputPath, OutputPath\n",
    "from ditwo.train import Trainer\n",
    "from ditwo.configs.config_dataclasses import DiTTrainingConfig, WAN_I2V_1_3B_CONFIG\n",
    "from ditwo.constants import WAN_FUN_I2V_1_3B_MULTIRES_MODEL_PATH\n",
    "from ditwo.utils.config import update_config\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
